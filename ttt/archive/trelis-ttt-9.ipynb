{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC AGI with Llama 3.2 1B - Test-time Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and re-format the raw data\n",
    "\n",
    "Load the ARC tasks. We split tasks containing more than one test input into separate tasks to make the pipeline easier.\n",
    "\n",
    "Data uploads:\n",
    "1. If you're running this on a remote GPU and have uploaded this notebook, you'll need to upload the raw arc json files as well.\n",
    "2. To run on a smaller split, also upload `mit-easy.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install uv -q\n",
    "!uv pip install pandas datasets tensorboard matplotlib numpy -qU --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arc_dataset_loader.py\n",
    "====================\n",
    "Utility helpers to load the ARC‚ÄëAGI training and evaluation splits as\n",
    "Hugging‚ÄØFace `datasets.Dataset` objects.\n",
    "\n",
    "Features\n",
    "--------\n",
    "* **Single public entry point** ‚Äì `get_arc_datasets` returns two datasets\n",
    "  (`arc_train`, `arc_eval`).\n",
    "* **Optional sub‚Äësampling** of either split via a JSON file containing a\n",
    "  list of problem IDs (e.g. those from `mit-easy.json`).\n",
    "* Correctly **fans‚Äëout tasks that contain multiple test IO pairs** so that\n",
    "  every row has exactly one test case ‚Äì exactly mirroring the logic used\n",
    "  during scoring.\n",
    "* **Lazy file resolution** ‚Äì only the files required for each split are\n",
    "  touched; just point `data_dir` at the location that holds the\n",
    "  `arc-agi_*` JSON files.\n",
    "\n",
    "Example\n",
    "~~~~~~~\n",
    ">>> from arc_dataset_loader import get_arc_datasets\n",
    ">>> train_ds, eval_ds = get_arc_datasets(\n",
    "...     data_dir=\"/path/to/data\",\n",
    "...     eval_subsample_json=\"mit-easy.json\",\n",
    "... )\n",
    ">>> print(len(train_ds), len(eval_ds))\n",
    "\n",
    "The returned objects are standard `datasets.Dataset` instances and can be\n",
    "used in any HF preprocessing / dataloader pipeline.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "__all__ = [\n",
    "    \"get_arc_datasets\",\n",
    "]\n",
    "\n",
    "###############################################################################\n",
    "# Internal helpers\n",
    "###############################################################################\n",
    "\n",
    "def _split_dictionary(data: Dict[str, Dict]) -> Tuple[Dict[str, Dict], List[str]]:\n",
    "    \"\"\"Split tasks that have *multiple* test IO pairs into separate entries.\n",
    "\n",
    "    Each new key is suffixed with an integer index (e.g. `\"abcd1234_1\"`).\n",
    "\n",
    "    Returns the revamped dictionary **and** a list with the names of the\n",
    "    expanded tasks (useful for debugging).\n",
    "    \"\"\"\n",
    "    result: Dict[str, Dict] = {}\n",
    "    split_files: List[str] = []\n",
    "\n",
    "    for key, value in data.items():\n",
    "        test_list = value.get(\"test\", [])\n",
    "        train_list = value.get(\"train\", [])\n",
    "\n",
    "        if len(test_list) > 1:\n",
    "            for idx, test_item in enumerate(test_list):\n",
    "                new_key = f\"{key}_{idx}\"\n",
    "                result[new_key] = {\"test\": [test_item], \"train\": train_list}\n",
    "                split_files.append(new_key)\n",
    "        else:\n",
    "            # untouched ‚Äì already a single‚Äëcase task\n",
    "            result[key] = value\n",
    "\n",
    "    return result, split_files\n",
    "\n",
    "\n",
    "def _build_dataframe(\n",
    "    *,\n",
    "    challenges: Dict[str, Dict],\n",
    "    solutions: Optional[Dict[str, List]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Construct the canonical DataFrame used for both splits.\"\"\"\n",
    "    data_rows: List[Dict] = []\n",
    "\n",
    "    for file_name, grids in challenges.items():\n",
    "        train_grids = grids.get(\"train\", [])\n",
    "        test_inputs = grids.get(\"test\", [])\n",
    "\n",
    "        # If solutions are provided (train and evaluation splits) we harvest\n",
    "        # the correct test outputs. When we created `challenges` we already\n",
    "        # fan‚Äëout multi‚Äëcase tasks, so every entry has exactly one test item.\n",
    "        if solutions is not None:\n",
    "            parts = file_name.split(\"_\")\n",
    "            base_key, test_idx = parts[0], int(parts[1] if len(parts) > 1 else 0)\n",
    "            correct_outputs = solutions.get(base_key, [])\n",
    "            # Guard: some evaluation sets may intentionally omit solutions\n",
    "            if test_idx >= len(correct_outputs):\n",
    "                raise ValueError(\n",
    "                    f\"No solution available for {file_name} (idx {test_idx}).\"\n",
    "                )\n",
    "            test_output = [{\"output\": correct_outputs[test_idx]}]\n",
    "        else:\n",
    "            test_output = []  # unknown at inference time\n",
    "\n",
    "        combined_test = (\n",
    "            [\n",
    "                {\n",
    "                    \"input\": test_inputs[0][\"input\"],\n",
    "                    \"output\": test_output[0][\"output\"],\n",
    "                }\n",
    "            ]\n",
    "            if test_output\n",
    "            else test_inputs\n",
    "        )\n",
    "\n",
    "        data_rows.append(\n",
    "            {\n",
    "                \"file_name\": file_name,\n",
    "                \"train\": train_grids,\n",
    "                \"test_input\": test_inputs,\n",
    "                \"test_output\": test_output,\n",
    "                \"test\": combined_test,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "\n",
    "def _apply_subsample(df: pd.DataFrame, subsample_file: Path | None) -> pd.DataFrame:\n",
    "    \"\"\"Optionally filter rows by problem IDs listed in *subsample_file*.\"\"\"\n",
    "    if subsample_file is None:\n",
    "        return df\n",
    "\n",
    "    with subsample_file.open() as fp:\n",
    "        ids = [line.strip() for line in json.load(fp)]  # expects a JSON list\n",
    "\n",
    "    # The DataFrame rows may be *split* versions ‚Äì we harvest the *root* ID\n",
    "    return df[df[\"file_name\"].str.extract(r\"^([a-f0-9]+)\")[0].isin(ids)]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Public API\n",
    "###############################################################################\n",
    "\n",
    "def get_arc_datasets(\n",
    "    *,\n",
    "    data_dir: str | Path = \".\",  # directory holding the arc‚Äëagi_*.json files\n",
    "    eval_subsample_json: str | Path | None = None,\n",
    "    train_subsample_json: str | Path | None = None,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Return `(arc_train, arc_eval)` as `datasets.Dataset` objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir:\n",
    "        Base directory containing the official ARC‚ÄëAGI JSON files (training,\n",
    "        evaluation). Defaults to the current working directory.\n",
    "\n",
    "    eval_subsample_json / train_subsample_json:\n",
    "        Optional path(s) to a JSON file with a list of problem IDs that\n",
    "        should be kept. When *None* the full split is used.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    *We purposely do not expose the private‚Äëtest split here ‚Äì that file set\n",
    "    lacks ground‚Äëtruth solutions and must be handled separately.*\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    ###############  TRAIN SPLIT  ###########################################\n",
    "    with (data_dir / \"arc-agi_training_challenges.json\").open() as fp:\n",
    "        train_challenges = json.load(fp)\n",
    "    train_challenges, _ = _split_dictionary(train_challenges)\n",
    "\n",
    "    with (data_dir / \"arc-agi_training_solutions.json\").open() as fp:\n",
    "        train_solutions = json.load(fp)\n",
    "\n",
    "    train_df = _build_dataframe(\n",
    "        challenges=train_challenges,\n",
    "        solutions=train_solutions,\n",
    "    )\n",
    "    train_df = _apply_subsample(train_df, Path(train_subsample_json) if train_subsample_json else None)\n",
    "\n",
    "    ###############  EVAL SPLIT  ############################################\n",
    "    with (data_dir / \"arc-agi_evaluation_challenges.json\").open() as fp:\n",
    "        eval_challenges = json.load(fp)\n",
    "    eval_challenges, _ = _split_dictionary(eval_challenges)\n",
    "\n",
    "    # Evaluation split *does* ship with solutions ‚Äì they just aren‚Äôt public on\n",
    "    # Kaggle. Adjust the path below if you keep them elsewhere.\n",
    "    eval_sol_path = data_dir / \"arc-agi_evaluation_solutions.json\"\n",
    "    if eval_sol_path.exists():\n",
    "        with eval_sol_path.open() as fp:\n",
    "            eval_solutions = json.load(fp)\n",
    "    else:\n",
    "        eval_solutions = None  # e.g. you‚Äôre working on the competition\n",
    "\n",
    "    eval_df = _build_dataframe(\n",
    "        challenges=eval_challenges,\n",
    "        solutions=eval_solutions,\n",
    "    )\n",
    "    eval_df = _apply_subsample(eval_df, Path(eval_subsample_json) if eval_subsample_json else None)\n",
    "\n",
    "    ###############  CONVERT ‚Üí HF DATASET  ###################################\n",
    "    arc_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    arc_eval = Dataset.from_pandas(eval_df.reset_index(drop=True))\n",
    "\n",
    "    return arc_train, arc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 416 training tasks and 20 evaluation tasks.\n"
     ]
    }
   ],
   "source": [
    "arc_train, arc_eval = get_arc_datasets(\n",
    "        # data_dir=args.data_dir, # comment out to use default\n",
    "        eval_subsample_json=\"mit-easy.json\", # comment out to use default of none\n",
    "        # train_subsample_json=args.train_subsample, # comment out to use default of none\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(arc_train):,} training tasks and {len(arc_eval):,} evaluation tasks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': '007bbfb7', 'train': [{'input': [[0, 7, 7], [7, 7, 7], [0, 7, 7]], 'output': [[0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 7, 7, 0, 7, 7, 0, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7], [0, 7, 7, 0, 7, 7, 0, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7]]}, {'input': [[4, 0, 4], [0, 0, 0], [0, 4, 0]], 'output': [[4, 0, 4, 0, 0, 0, 4, 0, 4], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 4, 0, 4, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0]]}, {'input': [[0, 0, 0], [0, 0, 2], [2, 0, 2]], 'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 2, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 2], [2, 0, 2, 0, 0, 0, 2, 0, 2]]}, {'input': [[6, 6, 0], [6, 0, 0], [0, 6, 6]], 'output': [[6, 6, 0, 6, 6, 0, 0, 0, 0], [6, 0, 0, 6, 0, 0, 0, 0, 0], [0, 6, 6, 0, 6, 6, 0, 0, 0], [6, 6, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0], [0, 6, 6, 0, 0, 0, 0, 0, 0], [0, 0, 0, 6, 6, 0, 6, 6, 0], [0, 0, 0, 6, 0, 0, 6, 0, 0], [0, 0, 0, 0, 6, 6, 0, 6, 6]]}, {'input': [[2, 2, 2], [0, 0, 0], [0, 2, 2]], 'output': [[2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 2, 0, 2, 2, 0, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 2, 2]]}], 'test_input': [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]]}], 'test_output': [{'output': [[7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 7, 0, 7, 7, 0, 0, 0, 0]]}], 'test': [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]], 'output': [[7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 7, 0, 7, 7, 0, 0, 0, 0]]}]}\n"
     ]
    }
   ],
   "source": [
    "print(arc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ARC-style palette (feel free to replace with your own)\n",
    "# 0-9 integers ‚Üí 10 RGB triples\n",
    "ARC_PALETTE = np.array([\n",
    "    [  0,   0,   0],   # 0 black\n",
    "    [255,   0,   0],   # 1 red\n",
    "    [  0, 255,   0],   # 2 green\n",
    "    [255, 255,   0],   # 3 yellow\n",
    "    [  0,   0, 255],   # 4 blue\n",
    "    [255,   0, 255],   # 5 magenta\n",
    "    [  0, 255, 255],   # 6 cyan\n",
    "    [255, 255, 255],   # 7 white\n",
    "    [128, 128, 128],   # 8 gray\n",
    "    [128,   0,   0],   # 9 dark-red (example)\n",
    "], dtype=np.uint8)\n",
    "ARC_CMAP = ListedColormap(ARC_PALETTE / 255.0, name='arc')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def plot_grid(grid, title='', ax=None, cmap=ARC_CMAP):\n",
    "    \"\"\"\n",
    "    Display an integer-labelled colour grid on the given Matplotlib axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid  : 2-D ndarray of ints 0-9\n",
    "    title : str\n",
    "    ax    : matplotlib.axes.Axes or None\n",
    "    cmap  : matplotlib.colors.Colormap\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.imshow(grid, interpolation='nearest', cmap=cmap, vmin=0, vmax=9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting inner index 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "challenge_index = 0\n",
    "challenge_split='train' # train or test\n",
    "split_index=0\n",
    "type='input' # input or output\n",
    "\n",
    "challenge = arc_train[challenge_index]\n",
    "print(f\"Selecting inner index {split_index}\")\n",
    "\n",
    "challenge_split = challenge[challenge_split]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "plot_grid(challenge_split[split_index]['input'],  'Input',  ax=axes[0])\n",
    "plot_grid(challenge_split[split_index]['output'], 'Output', ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grid = challenge_split[split_index][type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Evaluation and Fine-tuning Datasets\n",
    "- The evaluation dataset is for running an evaluation, not training. Use the ARC `eval` dataset here.\n",
    "- The fine-tuning dataset is for running a fine-tuning. You have a few options here and could use:\n",
    "    1. a synthetic ReARC dataset,\n",
    "    2. the ARC \"train\" dataset,\n",
    "    3. the ARC \"eval\" dataset, but with \"omit_test\" set to True so that test examples are not included. If you do this, you are doing test-time training (that is not task-specific).\n",
    "\n",
    "The dataset preparations functions will create a hf style dataset of messages, with a user message containing the first n-1 of n examples (inputs plus outputs) as train examples and the nth train input as the test example, and with an assistant message using the nth train output.\n",
    "\n",
    "When generating a fine-tuning dataset, you can optionally:\n",
    "- Expand that dataset via shuffling the order of examples.\n",
    "- Expand that dataset via colour shuffling.\n",
    "- Expand that dataset via rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprompt=\"\" # There is no point in providing a complicated pre-prompt\n",
    "# \"Given the following training examples with their input-output pairs, \"\n",
    "# \"predict the output for the test input based on the same \"\n",
    "# \"transformation rules:\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def prepare_evaluation_dataset(input_dataset, drop_first_train=False):\n",
    "    \"\"\"\n",
    "    Prepares evaluation datasets from the input dataset.\n",
    "    \n",
    "    Args:\n",
    "        input_dataset: A dataset containing 'file_name', 'train', and 'test' splits.\n",
    "                       Each 'train' entry contains 'input' and 'output' examples, and\n",
    "                       'test' contains 'input' and 'output' for evaluation.\n",
    "        drop_first_train (bool): Whether to drop the first training example. Defaults to False. Allows the training tasks to be shortened to shorten the context length.\n",
    "    \n",
    "    Returns:\n",
    "        - evaluation_dataset: Dataset formatted for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluation dataset preparation\n",
    "    evaluation_data = []\n",
    "    for challenge in input_dataset:\n",
    "        file_name = challenge['file_name']\n",
    "        train_examples = challenge['train']\n",
    "        test_example = challenge['test'][0]  # Use the first test example\n",
    "        \n",
    "        # Use all training examples as context, optionally dropping the first one\n",
    "        start_index = 1 if drop_first_train else 0\n",
    "        user_message_content = (\n",
    "            preprompt\n",
    "        )\n",
    "        for i, example in enumerate(train_examples[start_index:]):  # Include all training examples\n",
    "            user_message_content += (\n",
    "                f\"Input:\\n{np.array(example['input'])}\\n\"\n",
    "                f\"Output:\\n{np.array(example['output'])}\\n\\n\"\n",
    "            )\n",
    "        # Use the first test example's input as the test input\n",
    "        test_input = test_example['input']\n",
    "        user_message_content += f\"Test Input:\\n{np.array(test_input)}\\nTest Output:\\n\"\n",
    "        user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{np.array(test_example['output'])}\\n\\n\"\n",
    "        }\n",
    "\n",
    "        evaluation_data.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"messages\": [user_message, assistant_message]\n",
    "        })\n",
    "    \n",
    "    # Create HuggingFace Datasets\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_data)\n",
    "    \n",
    "    return evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Iterable, Union\n",
    "\n",
    "def prepare_fine_tuning_dataset(\n",
    "    input_dataset: Iterable[Dict],\n",
    "    *,\n",
    "    add_shuffled: Union[int, bool] = 0,   # 0/False ‚Üí none, True ‚Üí unlimited\n",
    "    add_rotations: bool = False,\n",
    "    add_mirrors: bool = False,\n",
    "    omit_test: bool = True,\n",
    "    apply_color_swaps: bool = False,\n",
    "    num_color_swaps: int = 2,\n",
    "    seed: int = 42,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a HuggingFace `Dataset` for ARC fine-tuning where **every row is a\n",
    "    multi-example chat prompt/response**:\n",
    "\n",
    "        ‚Ä¢ Context  = all but one example (train examples only, or train+test\n",
    "                     depending on `omit_test`)\n",
    "        ‚Ä¢ Query    = that held-out example (‚ÄúTest Input ‚Ä¶‚Äù)\n",
    "        ‚Ä¢ Answer   = its output grid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    add_shuffled   int  ‚Äì how many *extra* shuffled rows to add *per task*.\n",
    "                         0/False ‚Üí none.  True ‚Üí as many permutations as exist.\n",
    "                         For each such row we (a) choose a different held-out\n",
    "                         pair, (b) randomly shuffle the context order.\n",
    "    add_rotations / add_mirrors / apply_color_swaps\n",
    "                   If `True`, duplicate every (possibly shuffled) row with\n",
    "                   rotated / mirrored / colour-swapped variants.\n",
    "    omit_test      If False ‚ûú held-out pair is taken from `test`;  \n",
    "                   If True  ‚ûú held-out pair is taken from `train`.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helper transforms ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "    def rotate_grid(grid, angle: int):\n",
    "        if angle not in (90, 180, 270):\n",
    "            return grid\n",
    "        return np.rot90(np.array(grid), k=angle // 90).tolist()\n",
    "\n",
    "    def mirror_grid(grid, direction: str):\n",
    "        arr = np.array(grid)\n",
    "        if direction == \"horizontal\":\n",
    "            return np.fliplr(arr).tolist()\n",
    "        if direction == \"vertical\":\n",
    "            return np.flipud(arr).tolist()\n",
    "        return grid\n",
    "\n",
    "    def apply_mapping(grid, mapping_arr: np.ndarray):\n",
    "        return mapping_arr[np.array(grid)].tolist()\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ build ONE prompt/response row ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "    def make_row(file_tag: str,\n",
    "                 ctx_list: List[Dict],\n",
    "                 q_pair: Dict) -> Dict:\n",
    "        prompt = preprompt\n",
    "        for ex in ctx_list:\n",
    "            prompt += (\n",
    "                f\"Input:\\n{np.array(ex['input'])}\\n\"\n",
    "                f\"Output:\\n{np.array(ex['output'])}\\n\\n\"\n",
    "            )\n",
    "        prompt += f\"Test Input:\\n{np.array(q_pair['input'])}\\nTest Output:\\n\"\n",
    "\n",
    "        return {\n",
    "            \"file_name\": file_tag,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\",      \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{np.array(q_pair['output'])}\\n\\n\"},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ per task processing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "    def build_rows_for_task(file_name: str,\n",
    "                            train_ex: List[Dict],\n",
    "                            test_ex: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build one or more **multi-example** chat rows for a single ARC task.\n",
    "    \n",
    "        ‚Ä¢ If `omit_test=False` the pool of possible held-out pairs is\n",
    "          `test_ex + train_ex` (test examples first so the ‚Äúbase‚Äù row matches\n",
    "          evaluation).  \n",
    "        ‚Ä¢ If `omit_test=True`  the pool is `train_ex` only; every test example\n",
    "          is ignored.\n",
    "    \n",
    "        `add_shuffled` (int or bool) controls how many *extra* rows (beyond the\n",
    "        base row) are generated, each time choosing a different held-out pair\n",
    "        and shuffling the order of the remaining context examples.\n",
    "        \"\"\"\n",
    "        task_rows: List[Dict] = []\n",
    "    \n",
    "        # ---------- 1 ‚Äì assemble pool of held-out candidates ------------------ #\n",
    "        if omit_test or not test_ex:                     # ignore test examples\n",
    "            if len(train_ex) < 2:                        # need ‚â•2 to hold one out\n",
    "                return []                                # nothing we can do\n",
    "            candidates = train_ex                        # held-out comes from train\n",
    "        else:                                            # may hold out test *or* train\n",
    "            candidates = test_ex + train_ex              # test first ‚Üí base row identical\n",
    "    \n",
    "        # ---------- 2 ‚Äì decide how many *extra* rows we want ------------------ #\n",
    "        max_extra = len(candidates) - 1                  # cannot exceed pool size\n",
    "        if add_shuffled is True:                         # unlimited\n",
    "            want = max_extra\n",
    "        else:\n",
    "            want = int(add_shuffled) if add_shuffled else 0\n",
    "            want = min(want, max_extra)\n",
    "    \n",
    "        # ---------- 3 ‚Äì choose the held-out examples -------------------------- #\n",
    "        chosen_candidates = [candidates[0]]              # base row\n",
    "        remaining = candidates[1:]\n",
    "        if want:\n",
    "            rng.shuffle(remaining)\n",
    "            chosen_candidates += remaining[:want]\n",
    "    \n",
    "        # ---------- 4 ‚Äì build one row for each chosen candidate --------------- #\n",
    "        for idx, held_out in enumerate(chosen_candidates):\n",
    "            # context = all train examples except the held-out one (if it‚Äôs a train ex)\n",
    "            #         = all train examples               (if held-out is a test ex)\n",
    "            if held_out in train_ex:\n",
    "                context = [ex for ex in train_ex if ex is not held_out]\n",
    "            else:\n",
    "                context = train_ex\n",
    "    \n",
    "            # shuffle context order only in the *extra* rows\n",
    "            ctx_order = (rng.permutation(context).tolist()\n",
    "                         if (add_shuffled and idx > 0)\n",
    "                         else context)\n",
    "    \n",
    "            base_tag       = \"\" if idx == 0 else f\"_shuffle{idx}\"\n",
    "            base_file_name = f\"{file_name}{base_tag}\"\n",
    "    \n",
    "            # -- 4a. original orientation -------------------------------------- #\n",
    "            task_rows.append(make_row(base_file_name, ctx_order, held_out))\n",
    "    \n",
    "            # -- 4b. rotations -------------------------------------------------- #\n",
    "            if add_rotations:\n",
    "                for angle in (90, 180, 270):\n",
    "                    ctx_rot = [{\"input\":  rotate_grid(ex[\"input\"],  angle),\n",
    "                                \"output\": rotate_grid(ex[\"output\"], angle)}\n",
    "                               for ex in ctx_order]\n",
    "                    q_rot   = {\"input\":  rotate_grid(held_out[\"input\"],  angle),\n",
    "                               \"output\": rotate_grid(held_out[\"output\"], angle)}\n",
    "                    tag = f\"{base_file_name}_rot{angle}\"\n",
    "                    task_rows.append(make_row(tag, ctx_rot, q_rot))\n",
    "    \n",
    "            # -- 4c. mirrors ---------------------------------------------------- #\n",
    "            if add_mirrors:\n",
    "                for direction in (\"horizontal\", \"vertical\"):\n",
    "                    ctx_mir = [{\"input\":  mirror_grid(ex[\"input\"],  direction),\n",
    "                                \"output\": mirror_grid(ex[\"output\"], direction)}\n",
    "                               for ex in ctx_order]\n",
    "                    q_mir   = {\"input\":  mirror_grid(held_out[\"input\"],  direction),\n",
    "                               \"output\": mirror_grid(held_out[\"output\"], direction)}\n",
    "                    tag = f\"{base_file_name}_{direction}\"\n",
    "                    task_rows.append(make_row(tag, ctx_mir, q_mir))\n",
    "    \n",
    "        return task_rows\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ master pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #\n",
    "    for challenge in input_dataset:\n",
    "        file_name  = challenge[\"file_name\"]\n",
    "        base_train = challenge[\"train\"]\n",
    "        base_test  = challenge.get(\"test\", [])\n",
    "\n",
    "        # (1) original rows (plus rot/mirror)\n",
    "        rows.extend(build_rows_for_task(file_name, base_train, base_test))\n",
    "\n",
    "        # (2) global colour-swap augmentations\n",
    "        if apply_color_swaps and num_color_swaps > 0:\n",
    "            used_cols = {\n",
    "                c\n",
    "                for ex in base_train + base_test\n",
    "                for grid in (ex[\"input\"], ex[\"output\"])\n",
    "                for row in grid\n",
    "                for c in row\n",
    "            }\n",
    "\n",
    "            swaps_done = 0\n",
    "            while swaps_done < num_color_swaps:\n",
    "                perm = rng.permutation(10)\n",
    "                if all(perm[c] == c for c in used_cols):\n",
    "                    continue                          # nothing actually swapped\n",
    "\n",
    "                map_arr = perm\n",
    "                train_swapped = [\n",
    "                    {\"input\":  apply_mapping(ex[\"input\"],  map_arr),\n",
    "                     \"output\": apply_mapping(ex[\"output\"], map_arr)}\n",
    "                    for ex in deepcopy(base_train)\n",
    "                ]\n",
    "                test_swapped = [\n",
    "                    {\"input\":  apply_mapping(ex[\"input\"],  map_arr),\n",
    "                     \"output\": apply_mapping(ex[\"output\"], map_arr)}\n",
    "                    for ex in deepcopy(base_test)\n",
    "                ]\n",
    "\n",
    "                rows.extend(\n",
    "                    build_rows_for_task(\n",
    "                        f\"{file_name}_swap{swaps_done+1}\",\n",
    "                        train_swapped,\n",
    "                        test_swapped,\n",
    "                    )\n",
    "                )\n",
    "                swaps_done += 1\n",
    "\n",
    "    return Dataset.from_list(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 20\n",
      "})\n",
      "{'file_name': '00576224', 'messages': [{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Build the evaluation set (no augmentation, *with* test examples)\n",
    "eval_dataset = prepare_evaluation_dataset(\n",
    "    arc_eval,\n",
    ")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 832\n",
      "})\n",
      "{'file_name': '007bbfb7', 'messages': [{'content': 'Input:\\n[[0 7 7]\\n [7 7 7]\\n [0 7 7]]\\nOutput:\\n[[0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [7 7 7 7 7 7 7 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]]\\n\\nInput:\\n[[4 0 4]\\n [0 0 0]\\n [0 4 0]]\\nOutput:\\n[[4 0 4 0 0 0 4 0 4]\\n [0 0 0 0 0 0 0 0 0]\\n [0 4 0 0 0 0 0 4 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 4 0 4 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 4 0 0 0 0]]\\n\\nInput:\\n[[0 0 0]\\n [0 0 2]\\n [2 0 2]]\\nOutput:\\n[[0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 2]\\n [0 0 0 0 0 0 2 0 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 2 0 0 0 0 0 2]\\n [2 0 2 0 0 0 2 0 2]]\\n\\nInput:\\n[[6 6 0]\\n [6 0 0]\\n [0 6 6]]\\nOutput:\\n[[6 6 0 6 6 0 0 0 0]\\n [6 0 0 6 0 0 0 0 0]\\n [0 6 6 0 6 6 0 0 0]\\n [6 6 0 0 0 0 0 0 0]\\n [6 0 0 0 0 0 0 0 0]\\n [0 6 6 0 0 0 0 0 0]\\n [0 0 0 6 6 0 6 6 0]\\n [0 0 0 6 0 0 6 0 0]\\n [0 0 0 0 6 6 0 6 6]]\\n\\nInput:\\n[[2 2 2]\\n [0 0 0]\\n [0 2 2]]\\nOutput:\\n[[2 2 2 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 2 2 0 2 2 0 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 2 2 0 2 2]]\\n\\nTest Input:\\n[[7 0 7]\\n [7 0 7]\\n [7 7 0]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Build the training set (lots of augmentation, no test examples)\n",
    "fine_tuning_dataset = prepare_fine_tuning_dataset(\n",
    "    arc_train, # or set to arc_eval with omit_test=True\n",
    "    add_shuffled=1, #set to True for all permutations, or set to an integer for a max number of shuffles\n",
    "    add_rotations=False, # applies to original rotation only\n",
    "    add_mirrors=False, # applies to original examples only\n",
    "    apply_color_swaps=False,\n",
    "    num_color_swaps=1,\n",
    "    omit_test=False,\n",
    ")\n",
    "\n",
    "print(fine_tuning_dataset)\n",
    "print(fine_tuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune with Unsloth\n",
    "- Load unsloth\n",
    "- apply lora\n",
    "- train\n",
    "- Train on outputs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth accelerate hf_transfer\n",
    "\n",
    "## RESTART THE KERNEL if there are issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Standard import failed for UnslothPPOTrainer: No module named 'UnslothPPOTrainer'. Using tempfile instead!\n",
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 20000 # 8k needed for MIT easy eval. 20k needed for full MIT training split.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# del model, tokenizer\n",
    "\n",
    "model_slug = 'unsloth/Llama-3.2-1B' # can just use base model since we are simplifying the chat template anyways\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_slug,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Define the Jinja template\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "{{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "{% endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "[[8 6]\n",
      " [6 4]]\n",
      "Output:\n",
      "[[8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]\n",
      " [6 8 6 8 6 8]\n",
      " [4 6 4 6 4 6]\n",
      " [8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]]\n",
      "\n",
      "Input:\n",
      "[[7 9]\n",
      " [4 3]]\n",
      "Output:\n",
      "[[7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]\n",
      " [9 7 9 7 9 7]\n",
      " [3 4 3 4 3 4]\n",
      " [7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]]\n",
      "\n",
      "Test Input:\n",
      "[[3 2]\n",
      " [7 8]]\n",
      "Test Output:\n",
      "[[3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]\n",
      " [2 3 2 3 2 3]\n",
      " [8 7 8 7 8 7]\n",
      " [3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]]\n",
      "\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(eval_dataset[0]['messages'],tokenize=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_row_in_tokens(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the token length of the concatenated 'messages' content for each row\n",
    "    in the dataset and returns the length of the longest row.\n",
    "\n",
    "    Args:\n",
    "        dataset: A dataset with a 'messages' column containing lists of message dicts.\n",
    "        tokenizer: A tokenizer instance to tokenize the messages.\n",
    "\n",
    "    Returns:\n",
    "        int: The length in tokens of the longest row.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "\n",
    "    for row in dataset:\n",
    "        # Concatenate all message contents\n",
    "        concatenated_content = \" \".join(msg[\"content\"] for msg in row[\"messages\"])\n",
    "\n",
    "        # Tokenize the concatenated content and calculate its length\n",
    "        tokenized_length = len(tokenizer(concatenated_content)[\"input_ids\"])\n",
    "\n",
    "        # Track the maximum length\n",
    "        max_length = max(max_length, tokenized_length)\n",
    "\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18323"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_row_in_tokens(fine_tuning_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7423"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_row_in_tokens(eval_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.1 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # target_modules = [\"all-linear\"],\n",
    "    modules_to_save = [\"lm_head\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 832\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    \"\"\"\n",
    "    Turn every `messages` entry in a batch into a single, template-formatted string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        When `datasets.Dataset.map(..., batched=True)` is used, `examples[\"messages\"]`\n",
    "        is a list where each element is the message-history for one conversation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list whose length matches the input batch size.  Each item is the fully\n",
    "        formatted conversation produced by `tokenizer.apply_chat_template`.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tokenizer.apply_chat_template(\n",
    "            chat,                       # one conversation (list[dict])\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for chat in examples[\"messages\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n',\n",
       "  'role': 'user'},\n",
       " {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "[[8 6]\n",
      " [6 4]]\n",
      "Output:\n",
      "[[8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]\n",
      " [6 8 6 8 6 8]\n",
      " [4 6 4 6 4 6]\n",
      " [8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]]\n",
      "\n",
      "Input:\n",
      "[[7 9]\n",
      " [4 3]]\n",
      "Output:\n",
      "[[7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]\n",
      " [9 7 9 7 9 7]\n",
      " [3 4 3 4 3 4]\n",
      " [7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]]\n",
      "\n",
      "Test Input:\n",
      "[[3 2]\n",
      " [7 8]]\n",
      "Test Output:\n",
      "[[3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]\n",
      " [2 3 2 3 2 3]\n",
      " [8 7 8 7 8 7]\n",
      " [3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]]\n",
      "\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(formatting_func(eval_dataset)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cf9c1a52c644d7b4c91c26a9cb8300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2693e4aaa7504256a46d273f7c7ac5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "gradient_accumulation_steps=1 # avoids small errors with ga\n",
    "batch_size=16\n",
    "epochs=2\n",
    "\n",
    "def lr_lambda(current_step: int, num_training_steps: int):\n",
    "    if current_step < num_training_steps // 2:\n",
    "        return 1.0  # Constant learning rate for the first epoch\n",
    "    else:\n",
    "        # Cosine decay for the second epoch\n",
    "        progress = (current_step - num_training_steps // 2) / (num_training_steps // 2)\n",
    "        return 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)).item())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Calculate the total training steps\n",
    "num_training_steps = len(fine_tuning_dataset) * epochs // (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "# Create the scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, num_training_steps))\n",
    "\n",
    "# ----  SFTConfig replaces TrainingArguments  ----\n",
    "sft_cfg = SFTConfig(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_dir=\"logs\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=max(int(0.2 * num_training_steps),1.0),\n",
    "    logging_steps=max(int(0.1 * num_training_steps),1.0),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    optim=\"adamw_torch\",               # still needed so TRL knows which states to save\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # *everything that used to live at the top level*:\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Pass optimizer and scheduler explicitly to the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=fine_tuning_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_cfg,\n",
    "    formatting_func  = formatting_func,\n",
    "    # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    ")\n",
    "\n",
    "trainer.optimizer     = optimizer\n",
    "trainer.lr_scheduler  = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e51d0880154a6f8c193b8b7b8e8543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=192):   0%|          | 0/832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only # or run the code above if not using unsloth\n",
    "\n",
    "# masks everything between the instruction_part and response_part\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|begin_of_text|>\",\n",
    "    response_part = \"Test Output:\\n\",\n",
    "    # force_match=False # comment out to set true for a cleaner masking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let SFTTrainer handle the training and log the learning rate\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final memory results\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lora\n",
    "lora_model_name = f\"{model_slug.split('/')[-1]}-lora-model\"\n",
    "\n",
    "print(tokenizer.chat_template)\n",
    "\n",
    "model.save_pretrained(lora_model_name) # Local saving\n",
    "tokenizer.save_pretrained(lora_model_name) # Local saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Jinja template\n",
    "# tokenizer.chat_template = \"\"\"\n",
    "# {% for message in messages %}\n",
    "# {{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "# {% endfor %}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate using the base or fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Set the model ---\n",
    "# # del model, tokenizer\n",
    "\n",
    "# # model_slug = 'unsloth/Llama-3.2-1B' # using a base or instruct model won't work as there are no instructions on answer format provided in the prompt (to keep things succinct).\n",
    "# # model_slug = 'Trelis/Llama-3.2-1B-lora-model' # for the fine-tuned model\n",
    "# model_slug = lora_model_name\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# import torch\n",
    "# max_seq_length = 8000 # 8k needed for easy. 20k needed for full MIT split\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_slug,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def verify_vs_ground_truth(generated_output_text, correct_output, visualize=False):\n",
    "    \"\"\"Compares generated and correct outputs, handling diverse formats.\"\"\"\n",
    "    try:\n",
    "        # Preprocess generated output and correct output\n",
    "        def preprocess_output(output_text):\n",
    "            # Replace spaces between numbers with commas\n",
    "            cleaned_text = re.sub(r\"(?<=\\d)\\s+(?=\\d)\", \",\", output_text.strip())\n",
    "            # Replace newlines and spaces between rows with commas\n",
    "            cleaned_text = re.sub(r\"\\]\\s*\\[\", \"],[\", cleaned_text)\n",
    "            return cleaned_text\n",
    "\n",
    "        # Preprocess both generated and correct outputs\n",
    "        cleaned_generated_output = preprocess_output(generated_output_text)\n",
    "        cleaned_correct_output = preprocess_output(correct_output)\n",
    "\n",
    "        # Parse the cleaned outputs into NumPy arrays\n",
    "        processed_output = np.array(eval(cleaned_generated_output))\n",
    "        correct_output_array = np.array(eval(cleaned_correct_output))\n",
    "\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            plot_grid(processed_output,  'Generated Output',  ax=axes[0])\n",
    "            plot_grid(correct_output_array, 'Correct Output', ax=axes[1])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Validate shapes and broadcast if necessary\n",
    "        if processed_output.shape != correct_output_array.shape:\n",
    "            try:\n",
    "                processed_output = np.broadcast_to(processed_output, correct_output_array.shape)\n",
    "            except ValueError:\n",
    "                print(f\"Shape mismatch: Generated {processed_output.shape} vs Expected {correct_output_array.shape}\")\n",
    "                return False, 0.0, [[0]]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        total_pixels = correct_output_array.size\n",
    "        correct_pixels = np.sum(processed_output == correct_output_array)\n",
    "        accuracy = (correct_pixels / total_pixels) * 100\n",
    "        is_correct = correct_pixels == total_pixels\n",
    "\n",
    "        return is_correct, accuracy, processed_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {e}, Generated Output: {generated_output_text}\")\n",
    "        return False, 0.0, [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def solve_challenge_unsloth(messages, model, tokenizer, visualize=False):\n",
    "    \"\"\"Solves a single ARC challenge using unsloth for inference.\"\"\"\n",
    "\n",
    "    # print(messages)\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # prompt = tokenizer.apply_chat_template(\n",
    "    #     messages[:-1],\n",
    "    #     tokenize=False,\n",
    "    #     add_generation_prompt=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "\n",
    "    # print(prompt)\n",
    "    \n",
    "    try:\n",
    "        # Enable faster inference\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        \n",
    "        outputs = model.generate(input_ids = inputs,\n",
    "                                 max_new_tokens = 1500, # 1,500 for MIT easy.\n",
    "                                 use_cache = True,\n",
    "                                 temperature = 0.01)\n",
    "\n",
    "        generated_output = outputs[:, inputs.shape[1]:]\n",
    "\n",
    "        # Decode the output\n",
    "        generated_output_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]\n",
    "\n",
    "        # print(f\"Generated output text:\\n{generated_output_text}\")\n",
    "\n",
    "        # print(f\"Ground Truth:\\n{messages[-1]['content']}\")\n",
    "        \n",
    "        # Verify the generated output\n",
    "        is_correct, accuracy, processed_output = verify_vs_ground_truth(\n",
    "            generated_output_text, messages[-1]['content'], visualize=visualize\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'file_name': challenge['file_name'],\n",
    "            'generated_output': generated_output_text,\n",
    "            'is_correct': is_correct,\n",
    "            'accuracy': accuracy,\n",
    "            'processed_output': processed_output,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing challenge {challenge['file_name']}: {e}\")\n",
    "        return {\n",
    "            'file_name': challenge['file_name'],\n",
    "            'generated_output': \"Error\",\n",
    "            'is_correct': False,\n",
    "            'accuracy': 0.0,\n",
    "            'processed_output': [[0]],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 20\n",
    "\n",
    "dataset_to_run = eval_dataset.select(\n",
    "    # range(num_tests-3,num_tests)\n",
    "    range(num_tests)\n",
    ")\n",
    "\n",
    "print(dataset_to_run)\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    result = solve_challenge_unsloth(challenge['messages'], model, tokenizer, visualize=True)\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the correct count if the result is correct\n",
    "    if result.get('is_correct', False):\n",
    "        correct_count += 1\n",
    "\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} complete. Correct so far: {correct_count}/{i+1}.\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Depth-first Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------ #\n",
    "# Batched depth-first search helper (unchanged)                            #\n",
    "# ------------------------------------------------------------------------ #\n",
    "import math, heapq, torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def dfs_candidates(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.LongTensor,\n",
    "    max_tokens: int,\n",
    "    prob_thresh: float = 0.10,\n",
    "    top_k: int = 5,\n",
    "    batch_size: int = 64,\n",
    ") -> List[Tuple[torch.Tensor, float]]:\n",
    "    device   = prompt_ids.device\n",
    "    eos_id   = tokenizer.eos_token_id\n",
    "    log_cut  = math.log(prob_thresh)\n",
    "\n",
    "    frontier  = [(-0.0, prompt_ids)]\n",
    "    solutions = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while frontier:\n",
    "            batch = [heapq.heappop(frontier)\n",
    "                     for _ in range(min(batch_size, len(frontier)))]\n",
    "            neg_logps, seqs = zip(*batch)\n",
    "            logps   = torch.tensor([-x for x in neg_logps], device=device)\n",
    "            lens    = torch.tensor([s.shape[1] for s in seqs], device=device)\n",
    "\n",
    "            Lmax = int(lens.max())\n",
    "            padded = torch.full((len(seqs), Lmax),\n",
    "                                tokenizer.pad_token_id,\n",
    "                                device=device)\n",
    "            for i, s in enumerate(seqs):\n",
    "                padded[i, : s.shape[1]] = s\n",
    "\n",
    "            next_lp = torch.log_softmax(\n",
    "                model(padded).logits[torch.arange(len(seqs)), lens - 1],\n",
    "                dim=-1,\n",
    "            )                                               # (B, vocab)\n",
    "\n",
    "            for b, prefix in enumerate(seqs):\n",
    "                base_lp = logps[b].item()\n",
    "                vals, idxs = next_lp[b].topk(top_k)\n",
    "                for tok_lp, tok_id in zip(vals.tolist(), idxs.tolist()):\n",
    "                    new_lp = base_lp + tok_lp\n",
    "                    if new_lp < log_cut:\n",
    "                        break\n",
    "\n",
    "                    child = torch.cat(\n",
    "                        [prefix,\n",
    "                         torch.tensor([[tok_id]], device=device)], dim=1)\n",
    "\n",
    "                    done = (tok_id == eos_id or\n",
    "                            child.shape[1] - prompt_ids.shape[1] >= max_tokens)\n",
    "\n",
    "                    if done:\n",
    "                        solutions.append((child.clone(), new_lp))\n",
    "                    else:\n",
    "                        heapq.heappush(frontier, (-new_lp, child))\n",
    "\n",
    "    solutions.sort(key=lambda x: -x[1])\n",
    "    return solutions\n",
    "# ------------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------ #\n",
    "# Replacement for solve_challenge_unsloth                                  #\n",
    "# ------------------------------------------------------------------------ #\n",
    "def solve_challenge_unsloth_dfs(\n",
    "        messages,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        visualize=False,\n",
    "        *,\n",
    "        prob_thresh: float = 0.10,\n",
    "        top_k: int = 5,\n",
    "        batch_size: int = 64,\n",
    "        max_tokens: int = 930,\n",
    "):\n",
    "    \"\"\"\n",
    "    Same API as your original solver but uses DFS instead of model.generate.\n",
    "    \"\"\"\n",
    "    # 1) build prompt ids\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    try:\n",
    "        # 2) enumerate candidates\n",
    "        cands = dfs_candidates(\n",
    "            model, tokenizer, prompt_ids,\n",
    "            max_tokens=max_tokens,\n",
    "            prob_thresh=prob_thresh,\n",
    "            top_k=top_k,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        best_text, best_acc, best_proc, is_correct = None, -1.0, None, False\n",
    "\n",
    "        # 3) evaluate candidates\n",
    "        for seq, _ in cands:\n",
    "            gen_ids  = seq[:, prompt_ids.shape[1]:]\n",
    "            gen_text = tokenizer.decode(gen_ids[0],\n",
    "                                        skip_special_tokens=True)\n",
    "\n",
    "            ok, acc, proc = verify_vs_ground_truth(\n",
    "                gen_text, messages[-1]['content'],\n",
    "                visualize=visualize and ok)\n",
    "\n",
    "            if ok:                                   # perfect hit\n",
    "                best_text, best_acc, best_proc, is_correct = gen_text, acc, proc, True\n",
    "                break\n",
    "            if acc > best_acc:                       # keep best so far\n",
    "                best_text, best_acc, best_proc = gen_text, acc, proc\n",
    "\n",
    "        return {\n",
    "            \"generated_output\": best_text,\n",
    "            \"is_correct\":       is_correct,\n",
    "            \"accuracy\":         best_acc,\n",
    "            \"processed_output\": best_proc,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[DFS-solver] error: {e}\")\n",
    "        return {\n",
    "            \"generated_output\": \"Error\",\n",
    "            \"is_correct\": False,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"processed_output\": [[0]],\n",
    "        }\n",
    "# ------------------------------------------------------------------------ #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 20\n",
    "\n",
    "dataset_to_run = eval_dataset.select(\n",
    "    # range(num_tests-3,num_tests)\n",
    "    range(num_tests)\n",
    ")\n",
    "\n",
    "print(dataset_to_run)\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    result = solve_challenge_unsloth_dfs(\n",
    "            messages,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            visualize=True)\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the correct count if the result is correct\n",
    "    if result.get('is_correct', False):\n",
    "        correct_count += 1\n",
    "\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} complete. Correct so far: {correct_count}/{i+1}.\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Challenge-specific fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fine_tuning_dataset(fine_tuning_dataset, target_file_name):\n",
    "    \"\"\"\n",
    "    Filters the fine-tuning dataset to include only rows corresponding to the specified file name.\n",
    "\n",
    "    Args:\n",
    "        fine_tuning_dataset: A HuggingFace Dataset containing expanded fine-tuning data.\n",
    "        target_file_name: A string specifying the file name to filter by.\n",
    "\n",
    "    Returns:\n",
    "        A HuggingFace Dataset containing rows only for the specified file name.\n",
    "    \"\"\"\n",
    "    # Filter the dataset using a list comprehension\n",
    "    filtered_data = [\n",
    "        example for example in fine_tuning_dataset \n",
    "        if example['file_name'] == target_file_name\n",
    "    ]\n",
    "    \n",
    "    # Create a HuggingFace Dataset from the filtered data\n",
    "    filtered_dataset = Dataset.from_list(filtered_data)\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# lora_model_name = lora_model_name\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "batch_size=1\n",
    "epochs=2\n",
    "\n",
    "completions_only=True\n",
    "\n",
    "num_tests = 20 # 83 in full MIT split\n",
    "\n",
    "# Select the range of challenges to run\n",
    "dataset_to_run = eval_dataset.select(range(num_tests))\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    # Step 1: Filter the fine-tuning dataset for the current challenge\n",
    "    challenge_file_name = challenge['file_name']\n",
    "\n",
    "    challenge_dataset = filter_fine_tuning_dataset(fine_tuning_dataset, challenge_file_name)\n",
    "\n",
    "    # print(challenge_dataset)\n",
    "    \n",
    "    # print(challenge_file_name)\n",
    "\n",
    "    # del model, tokenizer\n",
    "    \n",
    "    # Step 2: Reload the model with the base LoRA\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=lora_model_name,  # Replace with your model's name\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable faster inference\n",
    "\n",
    "    # Define the Jinja template\n",
    "    tokenizer.chat_template = \"\"\"\n",
    "    {% for message in messages %}\n",
    "    {{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f\"Dataset size: {len(challenge_dataset)}\")\n",
    "    # print(f\"Epochs: {epochs}\")\n",
    "    # print(f\"Batch size: {batch_size}\")\n",
    "    # print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    # print(f\"Computed num_training_steps: {num_training_steps}\")\n",
    "    \n",
    "    def lr_lambda_specific(current_step: int, num_training_steps: int):\n",
    "        if num_training_steps < 2:\n",
    "            # If there are too few steps, return a constant learning rate\n",
    "            return 1.0\n",
    "        if current_step < num_training_steps // 2:\n",
    "            return 1.0  # Constant learning rate for the first half\n",
    "        else:\n",
    "            progress = (current_step - num_training_steps // 2) / max(1, (num_training_steps // 2))\n",
    "            return 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)).item())\n",
    "\n",
    "    # Step 3: Fine-tune the model on the challenge-specific dataset\n",
    "    # Define the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Calculate the total training steps\n",
    "    num_training_steps = len(fine_tuning_dataset) * epochs // (batch_size * gradient_accumulation_steps)\n",
    "    \n",
    "    # Create the scheduler\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, num_training_steps))\n",
    "    \n",
    "    # ----  SFTConfig replaces TrainingArguments  ----\n",
    "    sft_cfg = SFTConfig(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_strategy=\"steps\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=max(int(0.2 * num_training_steps),1.0),\n",
    "        logging_steps=max(int(0.1 * num_training_steps),1.0),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        optim=\"adamw_torch\",               # still needed so TRL knows which states to save\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"tensorboard\",\n",
    "        # *everything that used to live at the top level*:\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Pass optimizer and scheduler explicitly to the trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=fine_tuning_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=sft_cfg,\n",
    "        formatting_func  = formatting_func,\n",
    "        # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    )\n",
    "    \n",
    "    trainer.optimizer     = optimizer\n",
    "    trainer.lr_scheduler  = scheduler\n",
    "\n",
    "    if completions_only:\n",
    "        print(f\"TRAINING ON COMPLETIONS ONLY!\")\n",
    "        # Requires commenting in the datacollator above in the trainer.\n",
    "        from unsloth.chat_templates import train_on_responses_only # or run the code above if not using unsloth\n",
    "        \n",
    "        # masks everything between the instruction_part and response_part\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = \"<|begin_of_text|>\",\n",
    "            response_part = \"Test Output:\\n\",\n",
    "            # force_match=False # comment out to set true for a cleaner masking\n",
    "        )\n",
    "        \n",
    "        tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n",
    "        \n",
    "        space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "        tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n",
    "\n",
    "    trainer.train()  # Fine-tune the model\n",
    "\n",
    "    # Step 4: Run inference on the fine-tuned model\n",
    "    result = solve_challenge_unsloth(challenge['messages'], model, tokenizer, visualize=True)\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the correct count if the result is correct\n",
    "    if result.get('is_correct', False):\n",
    "        correct_count += 1\n",
    "\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} complete. Correct so far: {correct_count}/{i+1}.\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5123959,
     "sourceId": 8622192,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 89194,
     "modelInstanceId": 64831,
     "sourceId": 77114,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
