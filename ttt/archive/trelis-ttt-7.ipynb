{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC AGI with Llama 3.2 1B - Test-time Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and re-format the raw data\n",
    "\n",
    "Load the ARC tasks. We split tasks containing more than one test input into separate tasks to make the pipeline easier.\n",
    "\n",
    "Data uploads:\n",
    "1. If you're running this on a remote GPU and have uploaded this notebook, you'll need to upload the raw arc json files as well.\n",
    "2. To run on a smaller split, also upload `mit-easy.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install uv -q\n",
    "!uv pip install pandas datasets tensorboard matplotlib numpy -qU --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arc_dataset_loader.py\n",
    "====================\n",
    "Utility helpers to load the ARC‑AGI training and evaluation splits as\n",
    "Hugging Face `datasets.Dataset` objects.\n",
    "\n",
    "Features\n",
    "--------\n",
    "* **Single public entry point** – `get_arc_datasets` returns two datasets\n",
    "  (`arc_train`, `arc_eval`).\n",
    "* **Optional sub‑sampling** of either split via a JSON file containing a\n",
    "  list of problem IDs (e.g. those from `mit-easy.json`).\n",
    "* Correctly **fans‑out tasks that contain multiple test IO pairs** so that\n",
    "  every row has exactly one test case – exactly mirroring the logic used\n",
    "  during scoring.\n",
    "* **Lazy file resolution** – only the files required for each split are\n",
    "  touched; just point `data_dir` at the location that holds the\n",
    "  `arc-agi_*` JSON files.\n",
    "\n",
    "Example\n",
    "~~~~~~~\n",
    ">>> from arc_dataset_loader import get_arc_datasets\n",
    ">>> train_ds, eval_ds = get_arc_datasets(\n",
    "...     data_dir=\"/path/to/data\",\n",
    "...     eval_subsample_json=\"mit-easy.json\",\n",
    "... )\n",
    ">>> print(len(train_ds), len(eval_ds))\n",
    "\n",
    "The returned objects are standard `datasets.Dataset` instances and can be\n",
    "used in any HF preprocessing / dataloader pipeline.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "__all__ = [\n",
    "    \"get_arc_datasets\",\n",
    "]\n",
    "\n",
    "###############################################################################\n",
    "# Internal helpers\n",
    "###############################################################################\n",
    "\n",
    "def _split_dictionary(data: Dict[str, Dict]) -> Tuple[Dict[str, Dict], List[str]]:\n",
    "    \"\"\"Split tasks that have *multiple* test IO pairs into separate entries.\n",
    "\n",
    "    Each new key is suffixed with an integer index (e.g. `\"abcd1234_1\"`).\n",
    "\n",
    "    Returns the revamped dictionary **and** a list with the names of the\n",
    "    expanded tasks (useful for debugging).\n",
    "    \"\"\"\n",
    "    result: Dict[str, Dict] = {}\n",
    "    split_files: List[str] = []\n",
    "\n",
    "    for key, value in data.items():\n",
    "        test_list = value.get(\"test\", [])\n",
    "        train_list = value.get(\"train\", [])\n",
    "\n",
    "        if len(test_list) > 1:\n",
    "            for idx, test_item in enumerate(test_list):\n",
    "                new_key = f\"{key}_{idx}\"\n",
    "                result[new_key] = {\"test\": [test_item], \"train\": train_list}\n",
    "                split_files.append(new_key)\n",
    "        else:\n",
    "            # untouched – already a single‑case task\n",
    "            result[key] = value\n",
    "\n",
    "    return result, split_files\n",
    "\n",
    "\n",
    "def _build_dataframe(\n",
    "    *,\n",
    "    challenges: Dict[str, Dict],\n",
    "    solutions: Optional[Dict[str, List]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Construct the canonical DataFrame used for both splits.\"\"\"\n",
    "    data_rows: List[Dict] = []\n",
    "\n",
    "    for file_name, grids in challenges.items():\n",
    "        train_grids = grids.get(\"train\", [])\n",
    "        test_inputs = grids.get(\"test\", [])\n",
    "\n",
    "        # If solutions are provided (train and evaluation splits) we harvest\n",
    "        # the correct test outputs. When we created `challenges` we already\n",
    "        # fan‑out multi‑case tasks, so every entry has exactly one test item.\n",
    "        if solutions is not None:\n",
    "            parts = file_name.split(\"_\")\n",
    "            base_key, test_idx = parts[0], int(parts[1] if len(parts) > 1 else 0)\n",
    "            correct_outputs = solutions.get(base_key, [])\n",
    "            # Guard: some evaluation sets may intentionally omit solutions\n",
    "            if test_idx >= len(correct_outputs):\n",
    "                raise ValueError(\n",
    "                    f\"No solution available for {file_name} (idx {test_idx}).\"\n",
    "                )\n",
    "            test_output = [{\"output\": correct_outputs[test_idx]}]\n",
    "        else:\n",
    "            test_output = []  # unknown at inference time\n",
    "\n",
    "        combined_test = (\n",
    "            [\n",
    "                {\n",
    "                    \"input\": test_inputs[0][\"input\"],\n",
    "                    \"output\": test_output[0][\"output\"],\n",
    "                }\n",
    "            ]\n",
    "            if test_output\n",
    "            else test_inputs\n",
    "        )\n",
    "\n",
    "        data_rows.append(\n",
    "            {\n",
    "                \"file_name\": file_name,\n",
    "                \"train\": train_grids,\n",
    "                \"test_input\": test_inputs,\n",
    "                \"test_output\": test_output,\n",
    "                \"test\": combined_test,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "\n",
    "def _apply_subsample(df: pd.DataFrame, subsample_file: Path | None) -> pd.DataFrame:\n",
    "    \"\"\"Optionally filter rows by problem IDs listed in *subsample_file*.\"\"\"\n",
    "    if subsample_file is None:\n",
    "        return df\n",
    "\n",
    "    with subsample_file.open() as fp:\n",
    "        ids = [line.strip() for line in json.load(fp)]  # expects a JSON list\n",
    "\n",
    "    # The DataFrame rows may be *split* versions – we harvest the *root* ID\n",
    "    return df[df[\"file_name\"].str.extract(r\"^([a-f0-9]+)\")[0].isin(ids)]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Public API\n",
    "###############################################################################\n",
    "\n",
    "def get_arc_datasets(\n",
    "    *,\n",
    "    data_dir: str | Path = \".\",  # directory holding the arc‑agi_*.json files\n",
    "    eval_subsample_json: str | Path | None = None,\n",
    "    train_subsample_json: str | Path | None = None,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Return `(arc_train, arc_eval)` as `datasets.Dataset` objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir:\n",
    "        Base directory containing the official ARC‑AGI JSON files (training,\n",
    "        evaluation). Defaults to the current working directory.\n",
    "\n",
    "    eval_subsample_json / train_subsample_json:\n",
    "        Optional path(s) to a JSON file with a list of problem IDs that\n",
    "        should be kept. When *None* the full split is used.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    *We purposely do not expose the private‑test split here – that file set\n",
    "    lacks ground‑truth solutions and must be handled separately.*\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    ###############  TRAIN SPLIT  ###########################################\n",
    "    with (data_dir / \"arc-agi_training_challenges.json\").open() as fp:\n",
    "        train_challenges = json.load(fp)\n",
    "    train_challenges, _ = _split_dictionary(train_challenges)\n",
    "\n",
    "    with (data_dir / \"arc-agi_training_solutions.json\").open() as fp:\n",
    "        train_solutions = json.load(fp)\n",
    "\n",
    "    train_df = _build_dataframe(\n",
    "        challenges=train_challenges,\n",
    "        solutions=train_solutions,\n",
    "    )\n",
    "    train_df = _apply_subsample(train_df, Path(train_subsample_json) if train_subsample_json else None)\n",
    "\n",
    "    ###############  EVAL SPLIT  ############################################\n",
    "    with (data_dir / \"arc-agi_evaluation_challenges.json\").open() as fp:\n",
    "        eval_challenges = json.load(fp)\n",
    "    eval_challenges, _ = _split_dictionary(eval_challenges)\n",
    "\n",
    "    # Evaluation split *does* ship with solutions – they just aren’t public on\n",
    "    # Kaggle. Adjust the path below if you keep them elsewhere.\n",
    "    eval_sol_path = data_dir / \"arc-agi_evaluation_solutions.json\"\n",
    "    if eval_sol_path.exists():\n",
    "        with eval_sol_path.open() as fp:\n",
    "            eval_solutions = json.load(fp)\n",
    "    else:\n",
    "        eval_solutions = None  # e.g. you’re working on the competition\n",
    "\n",
    "    eval_df = _build_dataframe(\n",
    "        challenges=eval_challenges,\n",
    "        solutions=eval_solutions,\n",
    "    )\n",
    "    eval_df = _apply_subsample(eval_df, Path(eval_subsample_json) if eval_subsample_json else None)\n",
    "\n",
    "    ###############  CONVERT → HF DATASET  ###################################\n",
    "    arc_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    arc_eval = Dataset.from_pandas(eval_df.reset_index(drop=True))\n",
    "\n",
    "    return arc_train, arc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 416 training tasks and 20 evaluation tasks.\n"
     ]
    }
   ],
   "source": [
    "arc_train, arc_eval = get_arc_datasets(\n",
    "        # data_dir=args.data_dir, # comment out to use default\n",
    "        eval_subsample_json=\"mit-easy.json\", # comment out to use default of none\n",
    "        # train_subsample_json=args.train_subsample, # comment out to use default of none\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(arc_train):,} training tasks and {len(arc_eval):,} evaluation tasks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': '007bbfb7', 'train': [{'input': [[0, 7, 7], [7, 7, 7], [0, 7, 7]], 'output': [[0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 7, 7, 0, 7, 7, 0, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7], [0, 7, 7, 0, 7, 7, 0, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7]]}, {'input': [[4, 0, 4], [0, 0, 0], [0, 4, 0]], 'output': [[4, 0, 4, 0, 0, 0, 4, 0, 4], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 4, 0, 4, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0]]}, {'input': [[0, 0, 0], [0, 0, 2], [2, 0, 2]], 'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 2, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 2], [2, 0, 2, 0, 0, 0, 2, 0, 2]]}, {'input': [[6, 6, 0], [6, 0, 0], [0, 6, 6]], 'output': [[6, 6, 0, 6, 6, 0, 0, 0, 0], [6, 0, 0, 6, 0, 0, 0, 0, 0], [0, 6, 6, 0, 6, 6, 0, 0, 0], [6, 6, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0], [0, 6, 6, 0, 0, 0, 0, 0, 0], [0, 0, 0, 6, 6, 0, 6, 6, 0], [0, 0, 0, 6, 0, 0, 6, 0, 0], [0, 0, 0, 0, 6, 6, 0, 6, 6]]}, {'input': [[2, 2, 2], [0, 0, 0], [0, 2, 2]], 'output': [[2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 2, 0, 2, 2, 0, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 2, 2]]}], 'test_input': [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]]}], 'test_output': [{'output': [[7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 7, 0, 7, 7, 0, 0, 0, 0]]}], 'test': [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]], 'output': [[7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 7, 0, 7, 7, 0, 0, 0, 0]]}]}\n"
     ]
    }
   ],
   "source": [
    "print(arc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ARC-style palette (feel free to replace with your own)\n",
    "# 0-9 integers → 10 RGB triples\n",
    "ARC_PALETTE = np.array([\n",
    "    [  0,   0,   0],   # 0 black\n",
    "    [255,   0,   0],   # 1 red\n",
    "    [  0, 255,   0],   # 2 green\n",
    "    [255, 255,   0],   # 3 yellow\n",
    "    [  0,   0, 255],   # 4 blue\n",
    "    [255,   0, 255],   # 5 magenta\n",
    "    [  0, 255, 255],   # 6 cyan\n",
    "    [255, 255, 255],   # 7 white\n",
    "    [128, 128, 128],   # 8 gray\n",
    "    [128,   0,   0],   # 9 dark-red (example)\n",
    "], dtype=np.uint8)\n",
    "ARC_CMAP = ListedColormap(ARC_PALETTE / 255.0, name='arc')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def plot_grid(grid, title='', ax=None, cmap=ARC_CMAP):\n",
    "    \"\"\"\n",
    "    Display an integer-labelled colour grid on the given Matplotlib axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid  : 2-D ndarray of ints 0-9\n",
    "    title : str\n",
    "    ax    : matplotlib.axes.Axes or None\n",
    "    cmap  : matplotlib.colors.Colormap\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.imshow(grid, interpolation='nearest', cmap=cmap, vmin=0, vmax=9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting inner index 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "challenge_index = 0\n",
    "challenge_split='train' # train or test\n",
    "split_index=0\n",
    "type='input' # input or output\n",
    "\n",
    "challenge = arc_train[challenge_index]\n",
    "print(f\"Selecting inner index {split_index}\")\n",
    "\n",
    "challenge_split = challenge[challenge_split]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "plot_grid(challenge_split[split_index]['input'],  'Input',  ax=axes[0])\n",
    "plot_grid(challenge_split[split_index]['output'], 'Output', ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grid = challenge_split[split_index][type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Evaluation and Fine-tuning Datasets\n",
    "- The evaluation dataset is for running an evaluation, not training. Use the ARC `eval` dataset here.\n",
    "- The fine-tuning dataset is for running a fine-tuning. You have a few options here and could use:\n",
    "    1. a synthetic ReARC dataset,\n",
    "    2. the ARC \"train\" dataset,\n",
    "    3. the ARC \"eval\" dataset, but with \"omit_test\" set to True so that test examples are not included. If you do this, you are doing test-time training (that is not task-specific).\n",
    "\n",
    "The dataset preparations functions will create a hf style dataset of messages, with a user message containing the first n-1 of n examples (inputs plus outputs) as train examples and the nth train input as the test example, and with an assistant message using the nth train output.\n",
    "\n",
    "When generating a fine-tuning dataset, you can optionally:\n",
    "- Expand that dataset via shuffling the order of examples.\n",
    "- Expand that dataset via colour shuffling.\n",
    "- Expand that dataset via rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprompt=\"\" # There is no point in providing a complicated pre-prompt\n",
    "# \"Given the following training examples with their input-output pairs, \"\n",
    "# \"predict the output for the test input based on the same \"\n",
    "# \"transformation rules:\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def prepare_evaluation_dataset(input_dataset, drop_first_train=False):\n",
    "    \"\"\"\n",
    "    Prepares evaluation datasets from the input dataset.\n",
    "    \n",
    "    Args:\n",
    "        input_dataset: A dataset containing 'file_name', 'train', and 'test' splits.\n",
    "                       Each 'train' entry contains 'input' and 'output' examples, and\n",
    "                       'test' contains 'input' and 'output' for evaluation.\n",
    "        drop_first_train (bool): Whether to drop the first training example. Defaults to False. Allows the training tasks to be shortened to shorten the context length.\n",
    "    \n",
    "    Returns:\n",
    "        - evaluation_dataset: Dataset formatted for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluation dataset preparation\n",
    "    evaluation_data = []\n",
    "    for challenge in input_dataset:\n",
    "        file_name = challenge['file_name']\n",
    "        train_examples = challenge['train']\n",
    "        test_example = challenge['test'][0]  # Use the first test example\n",
    "        \n",
    "        # Use all training examples as context, optionally dropping the first one\n",
    "        start_index = 1 if drop_first_train else 0\n",
    "        user_message_content = (\n",
    "            preprompt\n",
    "        )\n",
    "        for i, example in enumerate(train_examples[start_index:]):  # Include all training examples\n",
    "            user_message_content += (\n",
    "                f\"Input:\\n{np.array(example['input'])}\\n\"\n",
    "                f\"Output:\\n{np.array(example['output'])}\\n\\n\"\n",
    "            )\n",
    "        # Use the first test example's input as the test input\n",
    "        test_input = test_example['input']\n",
    "        user_message_content += f\"Test Input:\\n{np.array(test_input)}\\nTest Output:\\n\"\n",
    "        user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{np.array(test_example['output'])}\\n\\n\"\n",
    "        }\n",
    "\n",
    "        evaluation_data.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"messages\": [user_message, assistant_message]\n",
    "        })\n",
    "    \n",
    "    # Create HuggingFace Datasets\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_data)\n",
    "    \n",
    "    return evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Iterable, Union\n",
    "\n",
    "def prepare_fine_tuning_dataset(\n",
    "    input_dataset: Iterable[Dict],\n",
    "    *,\n",
    "    add_shuffled: Union[int, bool] = 0,   # 0/False → none, True → unlimited\n",
    "    add_rotations: bool = False,\n",
    "    add_mirrors: bool = False,\n",
    "    omit_test: bool = True,\n",
    "    apply_color_swaps: bool = False,\n",
    "    num_color_swaps: int = 2,\n",
    "    seed: int = 42,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a HuggingFace `Dataset` for ARC fine-tuning where **every row is a\n",
    "    multi-example chat prompt/response**:\n",
    "\n",
    "        • Context  = all but one example (train examples only, or train+test\n",
    "                     depending on `omit_test`)\n",
    "        • Query    = that held-out example (“Test Input …”)\n",
    "        • Answer   = its output grid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    add_shuffled   int  – how many *extra* shuffled rows to add *per task*.\n",
    "                         0/False → none.  True → as many permutations as exist.\n",
    "                         For each such row we (a) choose a different held-out\n",
    "                         pair, (b) randomly shuffle the context order.\n",
    "    add_rotations / add_mirrors / apply_color_swaps\n",
    "                   If `True`, duplicate every (possibly shuffled) row with\n",
    "                   rotated / mirrored / colour-swapped variants.\n",
    "    omit_test      If False ➜ held-out pair is taken from `test`;  \n",
    "                   If True  ➜ held-out pair is taken from `train`.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    # ───────────────────────── helper transforms ──────────────────────────── #\n",
    "    def rotate_grid(grid, angle: int):\n",
    "        if angle not in (90, 180, 270):\n",
    "            return grid\n",
    "        return np.rot90(np.array(grid), k=angle // 90).tolist()\n",
    "\n",
    "    def mirror_grid(grid, direction: str):\n",
    "        arr = np.array(grid)\n",
    "        if direction == \"horizontal\":\n",
    "            return np.fliplr(arr).tolist()\n",
    "        if direction == \"vertical\":\n",
    "            return np.flipud(arr).tolist()\n",
    "        return grid\n",
    "\n",
    "    def apply_mapping(grid, mapping_arr: np.ndarray):\n",
    "        return mapping_arr[np.array(grid)].tolist()\n",
    "\n",
    "    # ────────────────────── build ONE prompt/response row ─────────────────── #\n",
    "    def make_row(file_tag: str,\n",
    "                 ctx_list: List[Dict],\n",
    "                 q_pair: Dict) -> Dict:\n",
    "        prompt = preprompt\n",
    "        for ex in ctx_list:\n",
    "            prompt += (\n",
    "                f\"Input:\\n{np.array(ex['input'])}\\n\"\n",
    "                f\"Output:\\n{np.array(ex['output'])}\\n\\n\"\n",
    "            )\n",
    "        prompt += f\"Test Input:\\n{np.array(q_pair['input'])}\\nTest Output:\\n\"\n",
    "\n",
    "        return {\n",
    "            \"file_name\": file_tag,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\",      \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{np.array(q_pair['output'])}\\n\\n\"},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    # ────────────────────────── per task processing ───────────────────────── #\n",
    "    def build_rows_for_task(file_name: str,\n",
    "                            train_ex: List[Dict],\n",
    "                            test_ex: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build one or more **multi-example** chat rows for a single ARC task.\n",
    "    \n",
    "        • If `omit_test=False` the pool of possible held-out pairs is\n",
    "          `test_ex + train_ex` (test examples first so the “base” row matches\n",
    "          evaluation).  \n",
    "        • If `omit_test=True`  the pool is `train_ex` only; every test example\n",
    "          is ignored.\n",
    "    \n",
    "        `add_shuffled` (int or bool) controls how many *extra* rows (beyond the\n",
    "        base row) are generated, each time choosing a different held-out pair\n",
    "        and shuffling the order of the remaining context examples.\n",
    "        \"\"\"\n",
    "        task_rows: List[Dict] = []\n",
    "    \n",
    "        # ---------- 1 – assemble pool of held-out candidates ------------------ #\n",
    "        if omit_test or not test_ex:                     # ignore test examples\n",
    "            if len(train_ex) < 2:                        # need ≥2 to hold one out\n",
    "                return []                                # nothing we can do\n",
    "            candidates = train_ex                        # held-out comes from train\n",
    "        else:                                            # may hold out test *or* train\n",
    "            candidates = test_ex + train_ex              # test first → base row identical\n",
    "    \n",
    "        # ---------- 2 – decide how many *extra* rows we want ------------------ #\n",
    "        max_extra = len(candidates) - 1                  # cannot exceed pool size\n",
    "        if add_shuffled is True:                         # unlimited\n",
    "            want = max_extra\n",
    "        else:\n",
    "            want = int(add_shuffled) if add_shuffled else 0\n",
    "            want = min(want, max_extra)\n",
    "    \n",
    "        # ---------- 3 – choose the held-out examples -------------------------- #\n",
    "        chosen_candidates = [candidates[0]]              # base row\n",
    "        remaining = candidates[1:]\n",
    "        if want:\n",
    "            rng.shuffle(remaining)\n",
    "            chosen_candidates += remaining[:want]\n",
    "    \n",
    "        # ---------- 4 – build one row for each chosen candidate --------------- #\n",
    "        for idx, held_out in enumerate(chosen_candidates):\n",
    "            # context = all train examples except the held-out one (if it’s a train ex)\n",
    "            #         = all train examples               (if held-out is a test ex)\n",
    "            if held_out in train_ex:\n",
    "                context = [ex for ex in train_ex if ex is not held_out]\n",
    "            else:\n",
    "                context = train_ex\n",
    "    \n",
    "            # shuffle context order only in the *extra* rows\n",
    "            ctx_order = (rng.permutation(context).tolist()\n",
    "                         if (add_shuffled and idx > 0)\n",
    "                         else context)\n",
    "    \n",
    "            base_tag       = \"\" if idx == 0 else f\"_shuffle{idx}\"\n",
    "            base_file_name = f\"{file_name}{base_tag}\"\n",
    "    \n",
    "            # -- 4a. original orientation -------------------------------------- #\n",
    "            task_rows.append(make_row(base_file_name, ctx_order, held_out))\n",
    "    \n",
    "            # -- 4b. rotations -------------------------------------------------- #\n",
    "            if add_rotations:\n",
    "                for angle in (90, 180, 270):\n",
    "                    ctx_rot = [{\"input\":  rotate_grid(ex[\"input\"],  angle),\n",
    "                                \"output\": rotate_grid(ex[\"output\"], angle)}\n",
    "                               for ex in ctx_order]\n",
    "                    q_rot   = {\"input\":  rotate_grid(held_out[\"input\"],  angle),\n",
    "                               \"output\": rotate_grid(held_out[\"output\"], angle)}\n",
    "                    tag = f\"{base_file_name}_rot{angle}\"\n",
    "                    task_rows.append(make_row(tag, ctx_rot, q_rot))\n",
    "    \n",
    "            # -- 4c. mirrors ---------------------------------------------------- #\n",
    "            if add_mirrors:\n",
    "                for direction in (\"horizontal\", \"vertical\"):\n",
    "                    ctx_mir = [{\"input\":  mirror_grid(ex[\"input\"],  direction),\n",
    "                                \"output\": mirror_grid(ex[\"output\"], direction)}\n",
    "                               for ex in ctx_order]\n",
    "                    q_mir   = {\"input\":  mirror_grid(held_out[\"input\"],  direction),\n",
    "                               \"output\": mirror_grid(held_out[\"output\"], direction)}\n",
    "                    tag = f\"{base_file_name}_{direction}\"\n",
    "                    task_rows.append(make_row(tag, ctx_mir, q_mir))\n",
    "    \n",
    "        return task_rows\n",
    "\n",
    "    # ─────────────────────────── master pipeline ──────────────────────────── #\n",
    "    for challenge in input_dataset:\n",
    "        file_name  = challenge[\"file_name\"]\n",
    "        base_train = challenge[\"train\"]\n",
    "        base_test  = challenge.get(\"test\", [])\n",
    "\n",
    "        # (1) original rows (plus rot/mirror)\n",
    "        rows.extend(build_rows_for_task(file_name, base_train, base_test))\n",
    "\n",
    "        # (2) global colour-swap augmentations\n",
    "        if apply_color_swaps and num_color_swaps > 0:\n",
    "            used_cols = {\n",
    "                c\n",
    "                for ex in base_train + base_test\n",
    "                for grid in (ex[\"input\"], ex[\"output\"])\n",
    "                for row in grid\n",
    "                for c in row\n",
    "            }\n",
    "\n",
    "            swaps_done = 0\n",
    "            while swaps_done < num_color_swaps:\n",
    "                perm = rng.permutation(10)\n",
    "                if all(perm[c] == c for c in used_cols):\n",
    "                    continue                          # nothing actually swapped\n",
    "\n",
    "                map_arr = perm\n",
    "                train_swapped = [\n",
    "                    {\"input\":  apply_mapping(ex[\"input\"],  map_arr),\n",
    "                     \"output\": apply_mapping(ex[\"output\"], map_arr)}\n",
    "                    for ex in deepcopy(base_train)\n",
    "                ]\n",
    "                test_swapped = [\n",
    "                    {\"input\":  apply_mapping(ex[\"input\"],  map_arr),\n",
    "                     \"output\": apply_mapping(ex[\"output\"], map_arr)}\n",
    "                    for ex in deepcopy(base_test)\n",
    "                ]\n",
    "\n",
    "                rows.extend(\n",
    "                    build_rows_for_task(\n",
    "                        f\"{file_name}_swap{swaps_done+1}\",\n",
    "                        train_swapped,\n",
    "                        test_swapped,\n",
    "                    )\n",
    "                )\n",
    "                swaps_done += 1\n",
    "\n",
    "    return Dataset.from_list(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 20\n",
      "})\n",
      "{'file_name': '00576224', 'messages': [{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Build the evaluation set (no augmentation, *with* test examples)\n",
    "eval_dataset = prepare_evaluation_dataset(\n",
    "    arc_eval,\n",
    ")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 832\n",
      "})\n",
      "{'file_name': '007bbfb7', 'messages': [{'content': 'Input:\\n[[0 7 7]\\n [7 7 7]\\n [0 7 7]]\\nOutput:\\n[[0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [7 7 7 7 7 7 7 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]]\\n\\nInput:\\n[[4 0 4]\\n [0 0 0]\\n [0 4 0]]\\nOutput:\\n[[4 0 4 0 0 0 4 0 4]\\n [0 0 0 0 0 0 0 0 0]\\n [0 4 0 0 0 0 0 4 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 4 0 4 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 4 0 0 0 0]]\\n\\nInput:\\n[[0 0 0]\\n [0 0 2]\\n [2 0 2]]\\nOutput:\\n[[0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 2]\\n [0 0 0 0 0 0 2 0 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 2 0 0 0 0 0 2]\\n [2 0 2 0 0 0 2 0 2]]\\n\\nInput:\\n[[6 6 0]\\n [6 0 0]\\n [0 6 6]]\\nOutput:\\n[[6 6 0 6 6 0 0 0 0]\\n [6 0 0 6 0 0 0 0 0]\\n [0 6 6 0 6 6 0 0 0]\\n [6 6 0 0 0 0 0 0 0]\\n [6 0 0 0 0 0 0 0 0]\\n [0 6 6 0 0 0 0 0 0]\\n [0 0 0 6 6 0 6 6 0]\\n [0 0 0 6 0 0 6 0 0]\\n [0 0 0 0 6 6 0 6 6]]\\n\\nInput:\\n[[2 2 2]\\n [0 0 0]\\n [0 2 2]]\\nOutput:\\n[[2 2 2 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 2 2 0 2 2 0 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 2 2 0 2 2]]\\n\\nTest Input:\\n[[7 0 7]\\n [7 0 7]\\n [7 7 0]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Build the training set (lots of augmentation, no test examples)\n",
    "fine_tuning_dataset = prepare_fine_tuning_dataset(\n",
    "    arc_train, # or set to arc_eval with omit_test=True\n",
    "    add_shuffled=1, #set to True for all permutations, or set to an integer for a max number of shuffles\n",
    "    add_rotations=False, # applies to original rotation only\n",
    "    add_mirrors=False, # applies to original examples only\n",
    "    apply_color_swaps=False,\n",
    "    num_color_swaps=1,\n",
    "    omit_test=False,\n",
    ")\n",
    "\n",
    "print(fine_tuning_dataset)\n",
    "print(fine_tuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune with Unsloth\n",
    "- Load unsloth\n",
    "- apply lora\n",
    "- train\n",
    "- Train on outputs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth accelerate hf_transfer\n",
    "\n",
    "## RESTART THE KERNEL if there are issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 20000 # 8k needed for MIT easy eval. 20k needed for full MIT training split.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "del model, tokenizer\n",
    "\n",
    "model_slug = 'unsloth/Llama-3.2-1B' # can just use base model since we are simplifying the chat template anyways\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_slug,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Define the Jinja template\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "{{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "{% endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "[[8 6]\n",
      " [6 4]]\n",
      "Output:\n",
      "[[8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]\n",
      " [6 8 6 8 6 8]\n",
      " [4 6 4 6 4 6]\n",
      " [8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]]\n",
      "\n",
      "Input:\n",
      "[[7 9]\n",
      " [4 3]]\n",
      "Output:\n",
      "[[7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]\n",
      " [9 7 9 7 9 7]\n",
      " [3 4 3 4 3 4]\n",
      " [7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]]\n",
      "\n",
      "Test Input:\n",
      "[[3 2]\n",
      " [7 8]]\n",
      "Test Output:\n",
      "[[3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]\n",
      " [2 3 2 3 2 3]\n",
      " [8 7 8 7 8 7]\n",
      " [3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]]\n",
      "\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(eval_dataset[0]['messages'],tokenize=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_row_in_tokens(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the token length of the concatenated 'messages' content for each row\n",
    "    in the dataset and returns the length of the longest row.\n",
    "\n",
    "    Args:\n",
    "        dataset: A dataset with a 'messages' column containing lists of message dicts.\n",
    "        tokenizer: A tokenizer instance to tokenize the messages.\n",
    "\n",
    "    Returns:\n",
    "        int: The length in tokens of the longest row.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "\n",
    "    for row in dataset:\n",
    "        # Concatenate all message contents\n",
    "        concatenated_content = \" \".join(msg[\"content\"] for msg in row[\"messages\"])\n",
    "\n",
    "        # Tokenize the concatenated content and calculate its length\n",
    "        tokenized_length = len(tokenizer(concatenated_content)[\"input_ids\"])\n",
    "\n",
    "        # Track the maximum length\n",
    "        max_length = max(max_length, tokenized_length)\n",
    "\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18323"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_row_in_tokens(fine_tuning_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7423"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_row_in_tokens(eval_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # target_modules = [\"all-linear\"],\n",
    "    modules_to_save = [\"lm_head\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 832\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    \"\"\"\n",
    "    Turn every `messages` entry in a batch into a single, template-formatted string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        When `datasets.Dataset.map(..., batched=True)` is used, `examples[\"messages\"]`\n",
    "        is a list where each element is the message-history for one conversation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list whose length matches the input batch size.  Each item is the fully\n",
    "        formatted conversation produced by `tokenizer.apply_chat_template`.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tokenizer.apply_chat_template(\n",
    "            chat,                       # one conversation (list[dict])\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for chat in examples[\"messages\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n',\n",
       "  'role': 'user'},\n",
       " {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "[[8 6]\n",
      " [6 4]]\n",
      "Output:\n",
      "[[8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]\n",
      " [6 8 6 8 6 8]\n",
      " [4 6 4 6 4 6]\n",
      " [8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]]\n",
      "\n",
      "Input:\n",
      "[[7 9]\n",
      " [4 3]]\n",
      "Output:\n",
      "[[7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]\n",
      " [9 7 9 7 9 7]\n",
      " [3 4 3 4 3 4]\n",
      " [7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]]\n",
      "\n",
      "Test Input:\n",
      "[[3 2]\n",
      " [7 8]]\n",
      "Test Output:\n",
      "[[3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]\n",
      " [2 3 2 3 2 3]\n",
      " [8 7 8 7 8 7]\n",
      " [3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]]\n",
      "\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(formatting_func(eval_dataset)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096237175898457e862b74da95d7a25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f879a34cf5407fa226151a9c6cf832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "batch_size=32\n",
    "epochs=2\n",
    "\n",
    "def lr_lambda(current_step: int, num_training_steps: int):\n",
    "    if current_step < num_training_steps // 2:\n",
    "        return 1.0  # Constant learning rate for the first epoch\n",
    "    else:\n",
    "        # Cosine decay for the second epoch\n",
    "        progress = (current_step - num_training_steps // 2) / (num_training_steps // 2)\n",
    "        return 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)).item())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Calculate the total training steps\n",
    "num_training_steps = len(fine_tuning_dataset) * epochs // (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "# Create the scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, num_training_steps))\n",
    "\n",
    "# ----  SFTConfig replaces TrainingArguments  ----\n",
    "sft_cfg = SFTConfig(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=max(int(0.2 * num_training_steps),1.0),\n",
    "    logging_steps=max(int(0.1 * num_training_steps),1.0),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    optim=\"adamw_torch\",               # still needed so TRL knows which states to save\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # *everything that used to live at the top level*:\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Pass optimizer and scheduler explicitly to the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=fine_tuning_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_cfg,\n",
    "    formatting_func  = formatting_func,\n",
    "    # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    ")\n",
    "\n",
    "trainer.optimizer     = optimizer\n",
    "trainer.lr_scheduler  = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7c40dd9d604ffc8715fe5375a5717e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=192):   0%|          | 0/832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 20. Reducing num_proc to 20 for dataset of size 20.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9292c1f1f19f4c8ba9ab73991bd6a388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only # or run the code above if not using unsloth\n",
    "\n",
    "# masks everything between the instruction_part and response_part\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|begin_of_text|>\",\n",
    "    response_part = \"Test Output:\\n\",\n",
    "    # force_match=False # comment out to set true for a cleaner masking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>\\nInput:\\n[[0 7 7]\\n [7 7 7]\\n [0 7 7]]\\nOutput:\\n[[0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [7 7 7 7 7 7 7 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]]\\n\\nInput:\\n[[4 0 4]\\n [0 0 0]\\n [0 4 0]]\\nOutput:\\n[[4 0 4 0 0 0 4 0 4]\\n [0 0 0 0 0 0 0 0 0]\\n [0 4 0 0 0 0 0 4 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 4 0 4 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 4 0 0 0 0]]\\n\\nInput:\\n[[0 0 0]\\n [0 0 2]\\n [2 0 2]]\\nOutput:\\n[[0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 2]\\n [0 0 0 0 0 0 2 0 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 2 0 0 0 0 0 2]\\n [2 0 2 0 0 0 2 0 2]]\\n\\nInput:\\n[[6 6 0]\\n [6 0 0]\\n [0 6 6]]\\nOutput:\\n[[6 6 0 6 6 0 0 0 0]\\n [6 0 0 6 0 0 0 0 0]\\n [0 6 6 0 6 6 0 0 0]\\n [6 6 0 0 0 0 0 0 0]\\n [6 0 0 0 0 0 0 0 0]\\n [0 6 6 0 0 0 0 0 0]\\n [0 0 0 6 6 0 6 6 0]\\n [0 0 0 6 0 0 6 0 0]\\n [0 0 0 0 6 6 0 6 6]]\\n\\nInput:\\n[[2 2 2]\\n [0 0 0]\\n [0 2 2]]\\nOutput:\\n[[2 2 2 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 2 2 0 2 2 0 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 2 2 0 2 2]]\\n\\nTest Input:\\n[[7 0 7]\\n [7 0 7]\\n [7 7 0]]\\nTest Output:\\n[[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n<|end_of_text|>'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n<|end_of_text|>'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.097 GB.\n",
      "61.594 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Check memory\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 832 | Num Epochs = 2 | Total steps = 52\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 547,880,960/2,046,363,648 (26.77% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/52 : < :, Epoch 0.04/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let SFTTrainer handle the training and log the learning rate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m/tmp/unsloth_compiled_cache/UnslothSFTTrainer.py:748\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 748\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py:1029\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:1200\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1199\u001b[0m ):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:1043\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:858\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m custom_forward\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_custom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_reentrant\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreserve_rng_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    867\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth_zoo/gradient_checkpointing.py:747\u001b[0m, in \u001b[0;36munsloth_checkpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    744\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    746\u001b[0m         )\n\u001b[0;32m--> 747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnslothCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    750\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    751\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth_zoo/gradient_checkpointing.py:463\u001b[0m, in \u001b[0;36mUnslothCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39m_requires_gradient: ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 463\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM\u001b[38;5;241m.\u001b[39mwait_stream(EXTRA_STREAM)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:855\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:559\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    558\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm, hidden_states)\n\u001b[0;32m--> 559\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/kernels/fast_lora.py:181\u001b[0m, in \u001b[0;36mapply_lora_mlp_swiglu\u001b[0;34m(self, X, inplace)\u001b[0m\n\u001b[1;32m    179\u001b[0m upW,     upW_quant,   upA,   upB,   upS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m  up_proj)\n\u001b[1;32m    180\u001b[0m downW, downW_quant, downA, downB, downS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj)\n\u001b[0;32m--> 181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mLoRA_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgateW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mupW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[43mupW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mupS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdownW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mswiglu_fg_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswiglu_DWf_DW_dfg_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                     \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:503\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled(device_type)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/kernels/fast_lora.py:75\u001b[0m, in \u001b[0;36mLoRA_MLP.forward\u001b[0;34m(ctx, X, gateW, gateW_quant, gateA, gateB, gateS, upW, upW_quant, upA, upB, upS, downW, downW_quant, downA, downB, downS, _forward_function, _backward_function, inplace)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@torch_amp_custom_fwd\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, X : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m             _forward_function, _backward_function,\n\u001b[1;32m     72\u001b[0m             inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,):\n\u001b[1;32m     73\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 75\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     g \u001b[38;5;241m=\u001b[39m matmul_lora(X,   upW,   upW_quant,   upA,   upB,   upS)\n\u001b[1;32m     77\u001b[0m     h \u001b[38;5;241m=\u001b[39m _forward_function(e, g)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/kernels/utils.py:492\u001b[0m, in \u001b[0;36mmatmul_lora\u001b[0;34m(X, W, W_quant, A, B, s, out)\u001b[0m\n\u001b[1;32m    490\u001b[0m     A, B \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mt(), B\u001b[38;5;241m.\u001b[39mt()\n\u001b[1;32m    491\u001b[0m     XA \u001b[38;5;241m=\u001b[39m torch_matmul(X, A\u001b[38;5;241m.\u001b[39mto(dtype))\n\u001b[0;32m--> 492\u001b[0m     out\u001b[38;5;241m.\u001b[39maddmm_(XA, \u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m, alpha \u001b[38;5;241m=\u001b[39m s)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# out += (X @ A.to(dtype)) @ (s * B.to(dtype))\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Let SFTTrainer handle the training and log the learning rate\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final memory results\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lora\n",
    "lora_model_name = f\"{model_slug.split('/')[-1]}-lora-model\"\n",
    "\n",
    "print(tokenizer.chat_template)\n",
    "\n",
    "model.save_pretrained(lora_model_name) # Local saving\n",
    "tokenizer.save_pretrained(lora_model_name) # Local saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Jinja template\n",
    "# tokenizer.chat_template = \"\"\"\n",
    "# {% for message in messages %}\n",
    "# {{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "# {% endfor %}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate using the base or fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Set the model ---\n",
    "# # del model, tokenizer\n",
    "\n",
    "# # model_slug = 'unsloth/Llama-3.2-1B' # using a base or instruct model won't work as there are no instructions on answer format provided in the prompt (to keep things succinct).\n",
    "# # model_slug = 'Trelis/Llama-3.2-1B-lora-model' # for the fine-tuned model\n",
    "# model_slug = lora_model_name\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# import torch\n",
    "# max_seq_length = 8000 # 8k needed for easy. 20k needed for full MIT split\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_slug,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def verify_vs_ground_truth(generated_output_text, correct_output, visualize=False):\n",
    "    \"\"\"Compares generated and correct outputs, handling diverse formats.\"\"\"\n",
    "    try:\n",
    "        # Preprocess generated output and correct output\n",
    "        def preprocess_output(output_text):\n",
    "            # Replace spaces between numbers with commas\n",
    "            cleaned_text = re.sub(r\"(?<=\\d)\\s+(?=\\d)\", \",\", output_text.strip())\n",
    "            # Replace newlines and spaces between rows with commas\n",
    "            cleaned_text = re.sub(r\"\\]\\s*\\[\", \"],[\", cleaned_text)\n",
    "            return cleaned_text\n",
    "\n",
    "        # Preprocess both generated and correct outputs\n",
    "        cleaned_generated_output = preprocess_output(generated_output_text)\n",
    "        cleaned_correct_output = preprocess_output(correct_output)\n",
    "\n",
    "        # Parse the cleaned outputs into NumPy arrays\n",
    "        processed_output = np.array(eval(cleaned_generated_output))\n",
    "        correct_output_array = np.array(eval(cleaned_correct_output))\n",
    "\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            plot_grid(processed_output,  'Generated Output',  ax=axes[0])\n",
    "            plot_grid(correct_output_array, 'Correct Output', ax=axes[1])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Validate shapes and broadcast if necessary\n",
    "        if processed_output.shape != correct_output_array.shape:\n",
    "            try:\n",
    "                processed_output = np.broadcast_to(processed_output, correct_output_array.shape)\n",
    "            except ValueError:\n",
    "                print(f\"Shape mismatch: Generated {processed_output.shape} vs Expected {correct_output_array.shape}\")\n",
    "                return False, 0.0, [[0]]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        total_pixels = correct_output_array.size\n",
    "        correct_pixels = np.sum(processed_output == correct_output_array)\n",
    "        accuracy = (correct_pixels / total_pixels) * 100\n",
    "        is_correct = correct_pixels == total_pixels\n",
    "\n",
    "        return is_correct, accuracy, processed_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {e}, Generated Output: {generated_output_text}\")\n",
    "        return False, 0.0, [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def solve_challenge_unsloth(messages, model, tokenizer, visualize=False):\n",
    "    \"\"\"Solves a single ARC challenge using unsloth for inference.\"\"\"\n",
    "\n",
    "    # print(messages)\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # prompt = tokenizer.apply_chat_template(\n",
    "    #     messages[:-1],\n",
    "    #     tokenize=False,\n",
    "    #     add_generation_prompt=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "\n",
    "    # print(prompt)\n",
    "    \n",
    "    try:\n",
    "        # Enable faster inference\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        \n",
    "        outputs = model.generate(input_ids = inputs,\n",
    "                                 max_new_tokens = 1500, # 1,500 for MIT easy.\n",
    "                                 use_cache = True,\n",
    "                                 temperature = 0.01)\n",
    "\n",
    "        generated_output = outputs[:, inputs.shape[1]:]\n",
    "\n",
    "        # Decode the output\n",
    "        generated_output_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]\n",
    "\n",
    "        # print(f\"Generated output text:\\n{generated_output_text}\")\n",
    "\n",
    "        # print(f\"Ground Truth:\\n{messages[-1]['content']}\")\n",
    "        \n",
    "        # Verify the generated output\n",
    "        is_correct, accuracy, processed_output = verify_vs_ground_truth(\n",
    "            generated_output_text, messages[-1]['content'], visualize=visualize\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'file_name': challenge['file_name'],\n",
    "            'generated_output': generated_output_text,\n",
    "            'is_correct': is_correct,\n",
    "            'accuracy': accuracy,\n",
    "            'processed_output': processed_output,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing challenge {challenge['file_name']}: {e}\")\n",
    "        return {\n",
    "            'file_name': challenge['file_name'],\n",
    "            'generated_output': \"Error\",\n",
    "            'is_correct': False,\n",
    "            'accuracy': 0.0,\n",
    "            'processed_output': [[0]],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 20\n",
    "\n",
    "dataset_to_run = eval_dataset.select(\n",
    "    # range(num_tests-3,num_tests)\n",
    "    range(num_tests)\n",
    ")\n",
    "\n",
    "print(dataset_to_run)\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    result = solve_challenge_unsloth(challenge['messages'], model, tokenizer, visualize=True)\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the correct count if the result is correct\n",
    "    if result.get('is_correct', False):\n",
    "        correct_count += 1\n",
    "\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} complete. Correct so far: {correct_count}/{i+1}.\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Challenge-specific fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fine_tuning_dataset(fine_tuning_dataset, target_file_name):\n",
    "    \"\"\"\n",
    "    Filters the fine-tuning dataset to include only rows corresponding to the specified file name.\n",
    "\n",
    "    Args:\n",
    "        fine_tuning_dataset: A HuggingFace Dataset containing expanded fine-tuning data.\n",
    "        target_file_name: A string specifying the file name to filter by.\n",
    "\n",
    "    Returns:\n",
    "        A HuggingFace Dataset containing rows only for the specified file name.\n",
    "    \"\"\"\n",
    "    # Filter the dataset using a list comprehension\n",
    "    filtered_data = [\n",
    "        example for example in fine_tuning_dataset \n",
    "        if example['file_name'] == target_file_name\n",
    "    ]\n",
    "    \n",
    "    # Create a HuggingFace Dataset from the filtered data\n",
    "    filtered_dataset = Dataset.from_list(filtered_data)\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lora_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 39\u001b[0m\n\u001b[1;32m     29\u001b[0m challenge_dataset \u001b[38;5;241m=\u001b[39m filter_fine_tuning_dataset(fine_tuning_dataset, challenge_file_name)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# print(challenge_dataset)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(challenge_file_name)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Step 2: Reload the model with the base LoRA\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m---> 39\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[43mlora_model_name\u001b[49m,  \u001b[38;5;66;03m# Replace with your model's name\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m     41\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m     42\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39mload_in_4bit,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(model)  \u001b[38;5;66;03m# Enable faster inference\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Define the Jinja template\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lora_model_name' is not defined"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# lora_model_name = lora_model_name\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "batch_size=1\n",
    "epochs=2\n",
    "\n",
    "completions_only=True\n",
    "\n",
    "num_tests = 20 # 83 in full MIT split\n",
    "\n",
    "# Select the range of challenges to run\n",
    "dataset_to_run = eval_dataset.select(range(num_tests))\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    # Step 1: Filter the fine-tuning dataset for the current challenge\n",
    "    challenge_file_name = challenge['file_name']\n",
    "\n",
    "    challenge_dataset = filter_fine_tuning_dataset(fine_tuning_dataset, challenge_file_name)\n",
    "\n",
    "    # print(challenge_dataset)\n",
    "    \n",
    "    # print(challenge_file_name)\n",
    "\n",
    "    # del model, tokenizer\n",
    "    \n",
    "    # Step 2: Reload the model with the base LoRA\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=lora_model_name,  # Replace with your model's name\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable faster inference\n",
    "\n",
    "    # Define the Jinja template\n",
    "    tokenizer.chat_template = \"\"\"\n",
    "    {% for message in messages %}\n",
    "    {{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f\"Dataset size: {len(challenge_dataset)}\")\n",
    "    # print(f\"Epochs: {epochs}\")\n",
    "    # print(f\"Batch size: {batch_size}\")\n",
    "    # print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    # print(f\"Computed num_training_steps: {num_training_steps}\")\n",
    "    \n",
    "    def lr_lambda_specific(current_step: int, num_training_steps: int):\n",
    "        if num_training_steps < 2:\n",
    "            # If there are too few steps, return a constant learning rate\n",
    "            return 1.0\n",
    "        if current_step < num_training_steps // 2:\n",
    "            return 1.0  # Constant learning rate for the first half\n",
    "        else:\n",
    "            progress = (current_step - num_training_steps // 2) / max(1, (num_training_steps // 2))\n",
    "            return 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)).item())\n",
    "\n",
    "    # Step 3: Fine-tune the model on the challenge-specific dataset\n",
    "    # Define the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Calculate the total training steps\n",
    "    num_training_steps = len(fine_tuning_dataset) * epochs // (batch_size * gradient_accumulation_steps)\n",
    "    \n",
    "    # Create the scheduler\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: lr_lambda(step, num_training_steps))\n",
    "    \n",
    "    # ----  SFTConfig replaces TrainingArguments  ----\n",
    "    sft_cfg = SFTConfig(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_strategy=\"steps\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=max(int(0.2 * num_training_steps),1.0),\n",
    "        logging_steps=max(int(0.1 * num_training_steps),1.0),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        optim=\"adamw_torch\",               # still needed so TRL knows which states to save\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"tensorboard\",\n",
    "        # *everything that used to live at the top level*:\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Pass optimizer and scheduler explicitly to the trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=fine_tuning_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=sft_cfg,\n",
    "        formatting_func  = formatting_func,\n",
    "        # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    )\n",
    "    \n",
    "    trainer.optimizer     = optimizer\n",
    "    trainer.lr_scheduler  = scheduler\n",
    "\n",
    "    if completions_only:\n",
    "        print(f\"TRAINING ON COMPLETIONS ONLY!\")\n",
    "        # Requires commenting in the datacollator above in the trainer.\n",
    "        from unsloth.chat_templates import train_on_responses_only # or run the code above if not using unsloth\n",
    "        \n",
    "        # masks everything between the instruction_part and response_part\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = \"<|begin_of_text|>\",\n",
    "            response_part = \"Test Output:\\n\",\n",
    "            # force_match=False # comment out to set true for a cleaner masking\n",
    "        )\n",
    "        \n",
    "        tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n",
    "        \n",
    "        space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "        tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n",
    "\n",
    "    trainer.train()  # Fine-tune the model\n",
    "\n",
    "    # Step 4: Run inference on the fine-tuned model\n",
    "    result = solve_challenge_unsloth(challenge['messages'], model, tokenizer, visualize=True)\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the correct count if the result is correct\n",
    "    if result.get('is_correct', False):\n",
    "        correct_count += 1\n",
    "\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} complete. Correct so far: {correct_count}/{i+1}.\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Submit\n",
    "\n",
    "For submission you must create a file called 'submission.json' which should have the format as explained [here](www.kaggle.com/competitions/arc-prize-2024/overview/evaluation)\n",
    "\n",
    "`\n",
    "{\"00576224\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"009d5c81\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"12997ef3\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]},\n",
    "              {\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " ...\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.select(range(dataset_range))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_range' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m solution_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[43mdataset_range\u001b[49m))):\n\u001b[1;32m      4\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     gen_solution \u001b[38;5;241m=\u001b[39m task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_range' is not defined"
     ]
    }
   ],
   "source": [
    "solution_dict = {}\n",
    "\n",
    "for i, task in enumerate(dataset.select(range(dataset_range))):\n",
    "    file_name = task['file_name']\n",
    "\n",
    "    gen_solution = task[\"generated_solution\"]\n",
    "    # For now we only do one attempt\n",
    "    solution_dict[file_name] = [\n",
    "        {\n",
    "            \"attempt_1\": gen_solution,\n",
    "            \"attempt_2\": [[0, 0], [0, 0]]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Recombining the solutions for split files\n",
    "combined_solution_dict = {}\n",
    "combined_files = {}\n",
    "\n",
    "for file_name, attempts in solution_dict.items():\n",
    "    base_name = file_name.split('_')[0]\n",
    "    if base_name not in combined_solution_dict:\n",
    "        combined_solution_dict[base_name] = []\n",
    "        combined_files[base_name] = []\n",
    "    combined_solution_dict[base_name].extend(attempts)\n",
    "    if '_' in file_name:\n",
    "        combined_files[base_name].append(file_name)\n",
    "        \n",
    "# Printing which file names have been combined\n",
    "print(\"Files that have been combined:\")\n",
    "for base_name, files in combined_files.items():\n",
    "    if files:  # Print only if there are files that were combined\n",
    "        print(f\"{base_name}: {', '.join(files)}\")\n",
    "\n",
    "# We still need to fill in dummy solutions for the tasks we did not consider to make a valid submission:\n",
    "# Load the sample submission file\n",
    "if kaggle:\n",
    "    with open('/kaggle/input/arc-prize-2024/sample_submission.json') as f:\n",
    "        sample_submission = json.load(f)\n",
    "else:\n",
    "    with open('sample_submission.json') as f:\n",
    "        sample_submission = json.load(f)\n",
    "\n",
    "# Fill in all entries that are still missing from the sample_submission file\n",
    "for key, value in sample_submission.items():\n",
    "    if key not in combined_solution_dict:\n",
    "        combined_solution_dict[key] = value\n",
    "\n",
    "# Create submission\n",
    "with open(\"submission.json\", \"w\") as json_file:\n",
    "    json.dump(combined_solution_dict, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5123959,
     "sourceId": 8622192,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 89194,
     "modelInstanceId": 64831,
     "sourceId": 77114,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
