{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARC AGI with Llama 3.2 1B - Test-time Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and re-format the raw data\n",
    "\n",
    "Load the ARC tasks. We split tasks containing more than one test input into separate tasks to make the pipeline easier.\n",
    "\n",
    "Data uploads:\n",
    "1. If you're running this on a remote GPU and have uploaded this notebook, you'll need to upload the raw arc json files as well.\n",
    "2. To run on a smaller split, also upload `mit-easy.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install uv -q\n",
    "\n",
    "!uv pip install huggingface_hub -qU\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pandas datasets tensorboard matplotlib numpy -qU --system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper if we're reloading the model\n",
    "\n",
    "import torch, gc, inspect, sys\n",
    "\n",
    "def clear_old_model_refs():\n",
    "    \"\"\"\n",
    "    Delete `model` and `tokenizer` (if they exist) from the caller’s\n",
    "    local *and* global scope, then garbage-collect and free GPU cache.\n",
    "    \"\"\"\n",
    "\n",
    "    # ── figure out the caller’s frame ────────────────────────────\n",
    "    frm = inspect.currentframe().f_back\n",
    "    caller_locals  = frm.f_locals\n",
    "    caller_globals = frm.f_globals\n",
    "\n",
    "    for var in (\"model\", \"tokenizer\"):\n",
    "        if var in caller_locals:\n",
    "            try:\n",
    "                del caller_locals[var]\n",
    "                if var in sys.modules:   # rarely needed\n",
    "                    del sys.modules[var]\n",
    "                print(f\"deleted local  {var}\")\n",
    "            except Exception as e:\n",
    "                print(f\"could not delete local {var}: {e}\")\n",
    "\n",
    "        if var in caller_globals:\n",
    "            try:\n",
    "                del caller_globals[var]\n",
    "                print(f\"deleted global {var}\")\n",
    "            except Exception as e:\n",
    "                print(f\"could not delete global {var}: {e}\")\n",
    "\n",
    "    # ── Python & CUDA cleanup ───────────────────────────────────\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arc_dataset_loader.py\n",
    "====================\n",
    "Utility helpers to load the ARC‑AGI training and evaluation splits as\n",
    "Hugging Face `datasets.Dataset` objects.\n",
    "\n",
    "Features\n",
    "--------\n",
    "* **Single public entry point** – `get_arc_datasets` returns two datasets\n",
    "  (`arc_train`, `arc_eval`).\n",
    "* **Optional sub‑sampling** of either split via a JSON file containing a\n",
    "  list of problem IDs (e.g. those from `mit-easy.json`).\n",
    "* Correctly **fans‑out tasks that contain multiple test IO pairs** so that\n",
    "  every row has exactly one test case – exactly mirroring the logic used\n",
    "  during scoring.\n",
    "* **Lazy file resolution** – only the files required for each split are\n",
    "  touched; just point `data_dir` at the location that holds the\n",
    "  `arc-agi_*` JSON files.\n",
    "\n",
    "Example\n",
    "~~~~~~~\n",
    ">>> from arc_dataset_loader import get_arc_datasets\n",
    ">>> train_ds, eval_ds = get_arc_datasets(\n",
    "...     data_dir=\"/path/to/data\",\n",
    "...     eval_subsample_json=\"mit-easy.json\",\n",
    "... )\n",
    ">>> print(len(train_ds), len(eval_ds))\n",
    "\n",
    "The returned objects are standard `datasets.Dataset` instances and can be\n",
    "used in any HF preprocessing / dataloader pipeline.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "__all__ = [\n",
    "    \"get_arc_datasets\",\n",
    "]\n",
    "\n",
    "###############################################################################\n",
    "# Internal helpers\n",
    "###############################################################################\n",
    "\n",
    "def _split_dictionary(data: Dict[str, Dict]) -> Tuple[Dict[str, Dict], List[str]]:\n",
    "    \"\"\"Split tasks that have *multiple* test IO pairs into separate entries.\n",
    "\n",
    "    Each new key is suffixed with an integer index (e.g. `\"abcd1234_1\"`).\n",
    "\n",
    "    Returns the revamped dictionary **and** a list with the names of the\n",
    "    expanded tasks (useful for debugging).\n",
    "    \"\"\"\n",
    "    result: Dict[str, Dict] = {}\n",
    "    split_files: List[str] = []\n",
    "\n",
    "    for key, value in data.items():\n",
    "        test_list = value.get(\"test\", [])\n",
    "        train_list = value.get(\"train\", [])\n",
    "\n",
    "        if len(test_list) > 1:\n",
    "            for idx, test_item in enumerate(test_list):\n",
    "                new_key = f\"{key}_{idx}\"\n",
    "                result[new_key] = {\"test\": [test_item], \"train\": train_list}\n",
    "                split_files.append(new_key)\n",
    "        else:\n",
    "            # untouched – already a single‑case task\n",
    "            result[key] = value\n",
    "\n",
    "    return result, split_files\n",
    "\n",
    "\n",
    "def _build_dataframe(\n",
    "    *,\n",
    "    challenges: Dict[str, Dict],\n",
    "    solutions: Optional[Dict[str, List]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Construct the canonical DataFrame used for both splits.\"\"\"\n",
    "    data_rows: List[Dict] = []\n",
    "\n",
    "    for file_name, grids in challenges.items():\n",
    "        train_grids = grids.get(\"train\", [])\n",
    "        test_inputs = grids.get(\"test\", [])\n",
    "\n",
    "        # If solutions are provided (train and evaluation splits) we harvest\n",
    "        # the correct test outputs. When we created `challenges` we already\n",
    "        # fan‑out multi‑case tasks, so every entry has exactly one test item.\n",
    "        if solutions is not None:\n",
    "            parts = file_name.split(\"_\")\n",
    "            base_key, test_idx = parts[0], int(parts[1] if len(parts) > 1 else 0)\n",
    "            correct_outputs = solutions.get(base_key, [])\n",
    "            # Guard: some evaluation sets may intentionally omit solutions\n",
    "            if test_idx >= len(correct_outputs):\n",
    "                raise ValueError(\n",
    "                    f\"No solution available for {file_name} (idx {test_idx}).\"\n",
    "                )\n",
    "            test_output = [{\"output\": correct_outputs[test_idx]}]\n",
    "        else:\n",
    "            test_output = []  # unknown at inference time\n",
    "\n",
    "        combined_test = (\n",
    "            [\n",
    "                {\n",
    "                    \"input\": test_inputs[0][\"input\"],\n",
    "                    \"output\": test_output[0][\"output\"],\n",
    "                }\n",
    "            ]\n",
    "            if test_output\n",
    "            else test_inputs\n",
    "        )\n",
    "\n",
    "        data_rows.append(\n",
    "            {\n",
    "                \"file_name\": file_name,\n",
    "                \"train\": train_grids,\n",
    "                \"test_input\": test_inputs,\n",
    "                \"test_output\": test_output,\n",
    "                \"test\": combined_test,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "\n",
    "def _apply_subsample(df: pd.DataFrame, subsample_file: Path | None) -> pd.DataFrame:\n",
    "    \"\"\"Optionally filter rows by problem IDs listed in *subsample_file*.\"\"\"\n",
    "    if subsample_file is None:\n",
    "        return df\n",
    "\n",
    "    with subsample_file.open() as fp:\n",
    "        ids = [line.strip() for line in json.load(fp)]  # expects a JSON list\n",
    "\n",
    "    # The DataFrame rows may be *split* versions – we harvest the *root* ID\n",
    "    return df[df[\"file_name\"].str.extract(r\"^([a-f0-9]+)\")[0].isin(ids)]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Public API\n",
    "###############################################################################\n",
    "\n",
    "def get_arc_datasets(\n",
    "    *,\n",
    "    data_dir: str | Path = \".\",  # directory holding the arc‑agi_*.json files\n",
    "    eval_subsample_json: str | Path | None = None,\n",
    "    train_subsample_json: str | Path | None = None,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Return `(arc_train, arc_eval)` as `datasets.Dataset` objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir:\n",
    "        Base directory containing the official ARC‑AGI JSON files (training,\n",
    "        evaluation). Defaults to the current working directory.\n",
    "\n",
    "    eval_subsample_json / train_subsample_json:\n",
    "        Optional path(s) to a JSON file with a list of problem IDs that\n",
    "        should be kept. When *None* the full split is used.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    *We purposely do not expose the private‑test split here – that file set\n",
    "    lacks ground‑truth solutions and must be handled separately.*\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "\n",
    "    ###############  TRAIN SPLIT  ###########################################\n",
    "    with (data_dir / \"arc-agi_training_challenges.json\").open() as fp:\n",
    "        train_challenges = json.load(fp)\n",
    "    train_challenges, _ = _split_dictionary(train_challenges)\n",
    "\n",
    "    with (data_dir / \"arc-agi_training_solutions.json\").open() as fp:\n",
    "        train_solutions = json.load(fp)\n",
    "\n",
    "    train_df = _build_dataframe(\n",
    "        challenges=train_challenges,\n",
    "        solutions=train_solutions,\n",
    "    )\n",
    "    train_df = _apply_subsample(train_df, Path(train_subsample_json) if train_subsample_json else None)\n",
    "\n",
    "    ###############  EVAL SPLIT  ############################################\n",
    "    with (data_dir / \"arc-agi_evaluation_challenges.json\").open() as fp:\n",
    "        eval_challenges = json.load(fp)\n",
    "    eval_challenges, _ = _split_dictionary(eval_challenges)\n",
    "\n",
    "    # Evaluation split *does* ship with solutions – they just aren’t public on\n",
    "    # Kaggle. Adjust the path below if you keep them elsewhere.\n",
    "    eval_sol_path = data_dir / \"arc-agi_evaluation_solutions.json\"\n",
    "    if eval_sol_path.exists():\n",
    "        with eval_sol_path.open() as fp:\n",
    "            eval_solutions = json.load(fp)\n",
    "    else:\n",
    "        eval_solutions = None  # e.g. you’re working on the competition\n",
    "\n",
    "    eval_df = _build_dataframe(\n",
    "        challenges=eval_challenges,\n",
    "        solutions=eval_solutions,\n",
    "    )\n",
    "    eval_df = _apply_subsample(eval_df, Path(eval_subsample_json) if eval_subsample_json else None)\n",
    "\n",
    "    ###############  CONVERT → HF DATASET  ###################################\n",
    "    arc_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    arc_eval = Dataset.from_pandas(eval_df.reset_index(drop=True))\n",
    "\n",
    "    return arc_train, arc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 416 training tasks and 20 evaluation tasks.\n"
     ]
    }
   ],
   "source": [
    "arc_train, arc_eval = get_arc_datasets(\n",
    "        # data_dir=args.data_dir, # comment out to use default\n",
    "        eval_subsample_json=\"mit-easy.json\", # comment out to use default of none\n",
    "        # train_subsample_json=args.train_subsample, # comment out to use default of none\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(arc_train):,} training tasks and {len(arc_eval):,} evaluation tasks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': '007bbfb7', 'train': [{'input': [[0, 7, 7], [7, 7, 7], [0, 7, 7]], 'output': [[0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 7, 7, 0, 7, 7, 0, 7, 7], [7, 7, 7, 7, 7, 7, 7, 7, 7], [0, 7, 7, 0, 7, 7, 0, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7], [0, 0, 0, 7, 7, 7, 7, 7, 7], [0, 0, 0, 0, 7, 7, 0, 7, 7]]}, {'input': [[4, 0, 4], [0, 0, 0], [0, 4, 0]], 'output': [[4, 0, 4, 0, 0, 0, 4, 0, 4], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 4, 0, 0, 0, 0, 0, 4, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 4, 0, 4, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 4, 0, 0, 0, 0]]}, {'input': [[0, 0, 0], [0, 0, 2], [2, 0, 2]], 'output': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 2, 0, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 0, 0, 0, 0, 2], [2, 0, 2, 0, 0, 0, 2, 0, 2]]}, {'input': [[6, 6, 0], [6, 0, 0], [0, 6, 6]], 'output': [[6, 6, 0, 6, 6, 0, 0, 0, 0], [6, 0, 0, 6, 0, 0, 0, 0, 0], [0, 6, 6, 0, 6, 6, 0, 0, 0], [6, 6, 0, 0, 0, 0, 0, 0, 0], [6, 0, 0, 0, 0, 0, 0, 0, 0], [0, 6, 6, 0, 0, 0, 0, 0, 0], [0, 0, 0, 6, 6, 0, 6, 6, 0], [0, 0, 0, 6, 0, 0, 6, 0, 0], [0, 0, 0, 0, 6, 6, 0, 6, 6]]}, {'input': [[2, 2, 2], [0, 0, 0], [0, 2, 2]], 'output': [[2, 2, 2, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 2, 0, 2, 2, 0, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 2, 2, 2, 2, 2, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 2, 0, 2, 2]]}], 'test_input': [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]]}], 'test_output': [{'output': [[7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 7, 0, 7, 7, 0, 0, 0, 0]]}], 'test': [{'input': [[7, 0, 7], [7, 0, 7], [7, 7, 0]], 'output': [[7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 0, 7, 0, 0, 0, 7, 0, 7], [7, 7, 0, 0, 0, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 0, 7, 7, 0, 7, 0, 0, 0], [7, 7, 0, 7, 7, 0, 0, 0, 0]]}]}\n"
     ]
    }
   ],
   "source": [
    "print(arc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# ARC-style palette (feel free to replace with your own)\n",
    "# 0-9 integers → 10 RGB triples\n",
    "ARC_PALETTE = np.array([\n",
    "    [  0,   0,   0],   # 0 black\n",
    "    [255,   0,   0],   # 1 red\n",
    "    [  0, 255,   0],   # 2 green\n",
    "    [255, 255,   0],   # 3 yellow\n",
    "    [  0,   0, 255],   # 4 blue\n",
    "    [255,   0, 255],   # 5 magenta\n",
    "    [  0, 255, 255],   # 6 cyan\n",
    "    [255, 255, 255],   # 7 white\n",
    "    [128, 128, 128],   # 8 gray\n",
    "    [128,   0,   0],   # 9 dark-red (example)\n",
    "], dtype=np.uint8)\n",
    "ARC_CMAP = ListedColormap(ARC_PALETTE / 255.0, name='arc')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def plot_grid(grid, title='', ax=None, cmap=ARC_CMAP):\n",
    "    \"\"\"\n",
    "    Display an integer-labelled colour grid on the given Matplotlib axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid  : 2-D ndarray of ints 0-9\n",
    "    title : str\n",
    "    ax    : matplotlib.axes.Axes or None\n",
    "    cmap  : matplotlib.colors.Colormap\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.imshow(grid, interpolation='nearest', cmap=cmap, vmin=0, vmax=9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting inner index 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "challenge_index = 0\n",
    "challenge_split='train' # train or test\n",
    "split_index=0\n",
    "type='input' # input or output\n",
    "\n",
    "challenge = arc_train[challenge_index]\n",
    "print(f\"Selecting inner index {split_index}\")\n",
    "\n",
    "challenge_split = challenge[challenge_split]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "plot_grid(challenge_split[split_index]['input'],  'Input',  ax=axes[0])\n",
    "plot_grid(challenge_split[split_index]['output'], 'Output', ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "grid = challenge_split[split_index][type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Evaluation and Fine-tuning Datasets\n",
    "- The evaluation dataset is for running an evaluation, not training. Use the ARC `eval` dataset here.\n",
    "- The fine-tuning dataset is for running a fine-tuning. You have a few options here and could use:\n",
    "    1. a synthetic ReARC dataset,\n",
    "    2. the ARC \"train\" dataset,\n",
    "    3. the ARC \"eval\" dataset, but with \"omit_test\" set to True so that test examples are not included. If you do this, you are doing test-time training (that is not task-specific).\n",
    "\n",
    "The dataset preparations functions will create a hf style dataset of messages, with a user message containing the first n-1 of n examples (inputs plus outputs) as train examples and the nth train input as the test example, and with an assistant message using the nth train output.\n",
    "\n",
    "When generating a fine-tuning dataset, you can optionally:\n",
    "- Expand that dataset via shuffling the order of examples.\n",
    "- Expand that dataset via colour shuffling.\n",
    "- Expand that dataset via rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprompt=\"\" # There is no point in providing a complicated pre-prompt\n",
    "# \"Given the following training examples with their input-output pairs, \"\n",
    "# \"predict the output for the test input based on the same \"\n",
    "# \"transformation rules:\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def prepare_evaluation_dataset(input_dataset, drop_first_train=False):\n",
    "    \"\"\"\n",
    "    Prepares evaluation datasets from the input dataset.\n",
    "    \n",
    "    Args:\n",
    "        input_dataset: A dataset containing 'file_name', 'train', and 'test' splits.\n",
    "                       Each 'train' entry contains 'input' and 'output' examples, and\n",
    "                       'test' contains 'input' and 'output' for evaluation.\n",
    "        drop_first_train (bool): Whether to drop the first training example. Defaults to False. Allows the training tasks to be shortened to shorten the context length.\n",
    "    \n",
    "    Returns:\n",
    "        - evaluation_dataset: Dataset formatted for evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluation dataset preparation\n",
    "    evaluation_data = []\n",
    "    for challenge in input_dataset:\n",
    "        file_name = challenge['file_name']\n",
    "        train_examples = challenge['train']\n",
    "        test_example = challenge['test'][0]  # Use the first test example\n",
    "        \n",
    "        # Use all training examples as context, optionally dropping the first one\n",
    "        start_index = 1 if drop_first_train else 0\n",
    "        user_message_content = (\n",
    "            preprompt\n",
    "        )\n",
    "        for i, example in enumerate(train_examples[start_index:]):  # Include all training examples\n",
    "            user_message_content += (\n",
    "                f\"Input:\\n{np.array(example['input'])}\\n\"\n",
    "                f\"Output:\\n{np.array(example['output'])}\\n\\n\"\n",
    "            )\n",
    "        # Use the first test example's input as the test input\n",
    "        test_input = test_example['input']\n",
    "        user_message_content += f\"Test Input:\\n{np.array(test_input)}\\nTest Output:\\n\"\n",
    "        user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{np.array(test_example['output'])}\\n\\n\"\n",
    "        }\n",
    "\n",
    "        evaluation_data.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"messages\": [user_message, assistant_message]\n",
    "        })\n",
    "    \n",
    "    # Create HuggingFace Datasets\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_data)\n",
    "    \n",
    "    return evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Iterable, Union\n",
    "\n",
    "def prepare_fine_tuning_dataset(\n",
    "    input_dataset: Iterable[Dict],\n",
    "    *,\n",
    "    add_shuffled: Union[int, bool] = 0,   # 0/False → none, True → unlimited\n",
    "    add_rotations: bool = False,\n",
    "    add_mirrors: bool = False,\n",
    "    omit_test: bool = True,\n",
    "    apply_color_swaps: bool = False,\n",
    "    num_color_swaps: int = 2,\n",
    "    seed: int = 42,\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a HuggingFace `Dataset` for ARC fine-tuning where **every row is a\n",
    "    multi-example chat prompt/response**:\n",
    "\n",
    "        • Context  = all but one example (train examples only, or train+test\n",
    "                     depending on `omit_test`)\n",
    "        • Query    = that held-out example (“Test Input …”)\n",
    "        • Answer   = its output grid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    add_shuffled   int  – how many *extra* shuffled rows to add *per task*.\n",
    "                         0/False → none.  True → as many permutations as exist.\n",
    "                         For each such row we (a) choose a different held-out\n",
    "                         pair, (b) randomly shuffle the context order.\n",
    "    add_rotations / add_mirrors / apply_color_swaps\n",
    "                   If `True`, duplicate every (possibly shuffled) row with\n",
    "                   rotated / mirrored / colour-swapped variants.\n",
    "    omit_test      If False ➜ held-out pair is taken from `test`;  \n",
    "                   If True  ➜ held-out pair is taken from `train`.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    # ───────────────────────── helper transforms ──────────────────────────── #\n",
    "    def rotate_grid(grid, angle: int):\n",
    "        if angle not in (90, 180, 270):\n",
    "            return grid\n",
    "        return np.rot90(np.array(grid), k=angle // 90).tolist()\n",
    "\n",
    "    def mirror_grid(grid, direction: str):\n",
    "        arr = np.array(grid)\n",
    "        if direction == \"horizontal\":\n",
    "            return np.fliplr(arr).tolist()\n",
    "        if direction == \"vertical\":\n",
    "            return np.flipud(arr).tolist()\n",
    "        return grid\n",
    "\n",
    "    def apply_mapping(grid, mapping_arr: np.ndarray):\n",
    "        return mapping_arr[np.array(grid)].tolist()\n",
    "\n",
    "    # ────────────────────── build ONE prompt/response row ─────────────────── #\n",
    "    def make_row(file_tag: str,\n",
    "                 ctx_list: List[Dict],\n",
    "                 q_pair: Dict) -> Dict:\n",
    "        prompt = preprompt\n",
    "        for ex in ctx_list:\n",
    "            prompt += (\n",
    "                f\"Input:\\n{np.array(ex['input'])}\\n\"\n",
    "                f\"Output:\\n{np.array(ex['output'])}\\n\\n\"\n",
    "            )\n",
    "        prompt += f\"Test Input:\\n{np.array(q_pair['input'])}\\nTest Output:\\n\"\n",
    "\n",
    "        return {\n",
    "            \"file_name\": file_tag,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\",      \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{np.array(q_pair['output'])}\\n\\n\"},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    # ────────────────────────── per task processing ───────────────────────── #\n",
    "    def build_rows_for_task(file_name: str,\n",
    "                            train_ex: List[Dict],\n",
    "                            test_ex: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Build one or more **multi-example** chat rows for a single ARC task.\n",
    "    \n",
    "        • If `omit_test=False` the pool of possible held-out pairs is\n",
    "          `test_ex + train_ex` (test examples first so the “base” row matches\n",
    "          evaluation).  \n",
    "        • If `omit_test=True`  the pool is `train_ex` only; every test example\n",
    "          is ignored.\n",
    "    \n",
    "        `add_shuffled` (int or bool) controls how many *extra* rows (beyond the\n",
    "        base row) are generated, each time choosing a different held-out pair\n",
    "        and shuffling the order of the remaining context examples.\n",
    "        \"\"\"\n",
    "        task_rows: List[Dict] = []\n",
    "    \n",
    "        # ---------- 1 – assemble pool of held-out candidates ------------------ #\n",
    "        if omit_test or not test_ex:                     # ignore test examples\n",
    "            if len(train_ex) < 2:                        # need ≥2 to hold one out\n",
    "                return []                                # nothing we can do\n",
    "            candidates = train_ex                        # held-out comes from train\n",
    "        else:                                            # may hold out test *or* train\n",
    "            candidates = test_ex + train_ex              # test first → base row identical\n",
    "    \n",
    "        # ---------- 2 – decide how many *extra* rows we want ------------------ #\n",
    "        max_extra = len(candidates) - 1                  # cannot exceed pool size\n",
    "        if add_shuffled is True:                         # unlimited\n",
    "            want = max_extra\n",
    "        else:\n",
    "            want = int(add_shuffled) if add_shuffled else 0\n",
    "            want = min(want, max_extra)\n",
    "    \n",
    "        # ---------- 3 – choose the held-out examples -------------------------- #\n",
    "        chosen_candidates = [candidates[0]]              # base row\n",
    "        remaining = candidates[1:]\n",
    "        if want:\n",
    "            rng.shuffle(remaining)\n",
    "            chosen_candidates += remaining[:want]\n",
    "    \n",
    "        # ---------- 4 – build one row for each chosen candidate --------------- #\n",
    "        for idx, held_out in enumerate(chosen_candidates):\n",
    "            # context = all train examples except the held-out one (if it’s a train ex)\n",
    "            #         = all train examples               (if held-out is a test ex)\n",
    "            if held_out in train_ex:\n",
    "                context = [ex for ex in train_ex if ex is not held_out]\n",
    "            else:\n",
    "                context = train_ex\n",
    "    \n",
    "            # shuffle context order only in the *extra* rows\n",
    "            ctx_order = (rng.permutation(context).tolist()\n",
    "                         if (add_shuffled and idx > 0)\n",
    "                         else context)\n",
    "    \n",
    "            base_tag       = \"\" if idx == 0 else f\"_shuffle{idx}\"\n",
    "            base_file_name = f\"{file_name}{base_tag}\"\n",
    "    \n",
    "            # -- 4a. original orientation -------------------------------------- #\n",
    "            task_rows.append(make_row(base_file_name, ctx_order, held_out))\n",
    "    \n",
    "            # -- 4b. rotations -------------------------------------------------- #\n",
    "            if add_rotations:\n",
    "                for angle in (90, 180, 270):\n",
    "                    ctx_rot = [{\"input\":  rotate_grid(ex[\"input\"],  angle),\n",
    "                                \"output\": rotate_grid(ex[\"output\"], angle)}\n",
    "                               for ex in ctx_order]\n",
    "                    q_rot   = {\"input\":  rotate_grid(held_out[\"input\"],  angle),\n",
    "                               \"output\": rotate_grid(held_out[\"output\"], angle)}\n",
    "                    tag = f\"{base_file_name}_rot{angle}\"\n",
    "                    task_rows.append(make_row(tag, ctx_rot, q_rot))\n",
    "    \n",
    "            # -- 4c. mirrors ---------------------------------------------------- #\n",
    "            if add_mirrors:\n",
    "                for direction in (\"horizontal\", \"vertical\"):\n",
    "                    ctx_mir = [{\"input\":  mirror_grid(ex[\"input\"],  direction),\n",
    "                                \"output\": mirror_grid(ex[\"output\"], direction)}\n",
    "                               for ex in ctx_order]\n",
    "                    q_mir   = {\"input\":  mirror_grid(held_out[\"input\"],  direction),\n",
    "                               \"output\": mirror_grid(held_out[\"output\"], direction)}\n",
    "                    tag = f\"{base_file_name}_{direction}\"\n",
    "                    task_rows.append(make_row(tag, ctx_mir, q_mir))\n",
    "    \n",
    "        return task_rows\n",
    "\n",
    "    # ─────────────────────────── master pipeline ──────────────────────────── #\n",
    "    for challenge in input_dataset:\n",
    "        file_name  = challenge[\"file_name\"]\n",
    "        base_train = challenge[\"train\"]\n",
    "        base_test  = challenge.get(\"test\", [])\n",
    "\n",
    "        # (1) original rows (plus rot/mirror)\n",
    "        rows.extend(build_rows_for_task(file_name, base_train, base_test))\n",
    "\n",
    "        # (2) global colour-swap augmentations\n",
    "        if apply_color_swaps and num_color_swaps > 0:\n",
    "            used_cols = {\n",
    "                c\n",
    "                for ex in base_train + base_test\n",
    "                for grid in (ex[\"input\"], ex[\"output\"])\n",
    "                for row in grid\n",
    "                for c in row\n",
    "            }\n",
    "\n",
    "            swaps_done = 0\n",
    "            while swaps_done < num_color_swaps:\n",
    "                perm = rng.permutation(10)\n",
    "                if all(perm[c] == c for c in used_cols):\n",
    "                    continue                          # nothing actually swapped\n",
    "\n",
    "                map_arr = perm\n",
    "                train_swapped = [\n",
    "                    {\"input\":  apply_mapping(ex[\"input\"],  map_arr),\n",
    "                     \"output\": apply_mapping(ex[\"output\"], map_arr)}\n",
    "                    for ex in deepcopy(base_train)\n",
    "                ]\n",
    "                test_swapped = [\n",
    "                    {\"input\":  apply_mapping(ex[\"input\"],  map_arr),\n",
    "                     \"output\": apply_mapping(ex[\"output\"], map_arr)}\n",
    "                    for ex in deepcopy(base_test)\n",
    "                ]\n",
    "\n",
    "                rows.extend(\n",
    "                    build_rows_for_task(\n",
    "                        f\"{file_name}_swap{swaps_done+1}\",\n",
    "                        train_swapped,\n",
    "                        test_swapped,\n",
    "                    )\n",
    "                )\n",
    "                swaps_done += 1\n",
    "\n",
    "    return Dataset.from_list(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 20\n",
      "})\n",
      "{'file_name': '00576224', 'messages': [{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Build the evaluation set (no augmentation, *with* test examples)\n",
    "eval_dataset = prepare_evaluation_dataset(\n",
    "    arc_eval,\n",
    ")\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 9984\n",
      "})\n",
      "{'file_name': '007bbfb7', 'messages': [{'content': 'Input:\\n[[0 7 7]\\n [7 7 7]\\n [0 7 7]]\\nOutput:\\n[[0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [7 7 7 7 7 7 7 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]]\\n\\nInput:\\n[[4 0 4]\\n [0 0 0]\\n [0 4 0]]\\nOutput:\\n[[4 0 4 0 0 0 4 0 4]\\n [0 0 0 0 0 0 0 0 0]\\n [0 4 0 0 0 0 0 4 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 4 0 4 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 4 0 0 0 0]]\\n\\nInput:\\n[[0 0 0]\\n [0 0 2]\\n [2 0 2]]\\nOutput:\\n[[0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 2]\\n [0 0 0 0 0 0 2 0 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 2 0 0 0 0 0 2]\\n [2 0 2 0 0 0 2 0 2]]\\n\\nInput:\\n[[6 6 0]\\n [6 0 0]\\n [0 6 6]]\\nOutput:\\n[[6 6 0 6 6 0 0 0 0]\\n [6 0 0 6 0 0 0 0 0]\\n [0 6 6 0 6 6 0 0 0]\\n [6 6 0 0 0 0 0 0 0]\\n [6 0 0 0 0 0 0 0 0]\\n [0 6 6 0 0 0 0 0 0]\\n [0 0 0 6 6 0 6 6 0]\\n [0 0 0 6 0 0 6 0 0]\\n [0 0 0 0 6 6 0 6 6]]\\n\\nInput:\\n[[2 2 2]\\n [0 0 0]\\n [0 2 2]]\\nOutput:\\n[[2 2 2 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 2 2 0 2 2 0 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 2 2 0 2 2]]\\n\\nTest Input:\\n[[7 0 7]\\n [7 0 7]\\n [7 7 0]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Build the training set (lots of augmentation, no test examples)\n",
    "fine_tuning_dataset = prepare_fine_tuning_dataset(\n",
    "    arc_train, # or set to arc_eval with omit_test=True\n",
    "    # arc_eval,\n",
    "    omit_test=False,\n",
    "    add_shuffled=1, #set to True for all permutations, or set to an integer for a max number of shuffles\n",
    "    add_rotations=True, # applies to original rotation only\n",
    "    add_mirrors=True, # applies to original examples only\n",
    "    apply_color_swaps=True,\n",
    "    num_color_swaps=1,\n",
    ")\n",
    "\n",
    "print(fine_tuning_dataset)\n",
    "print(fine_tuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': '11852cab_swap1_shuffle1_vertical', 'messages': [{'content': 'Input:\\n[[3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 7 3 3 3 3 3]\\n [3 3 3 9 3 9 3 3 3 3]\\n [3 3 7 3 6 3 7 3 3 3]\\n [3 3 3 9 3 9 3 3 3 3]\\n [3 3 6 3 7 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]]\\nOutput:\\n[[3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 6 3 7 3 6 3 3 3]\\n [3 3 3 9 3 9 3 3 3 3]\\n [3 3 7 3 6 3 7 3 3 3]\\n [3 3 3 9 3 9 3 3 3 3]\\n [3 3 6 3 7 3 6 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]]\\n\\nInput:\\n[[3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 7 3 7 3 7 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 7 3 0 3 7 3 3]\\n [3 3 3 3 8 3 3 3 3 3]\\n [3 3 3 7 3 7 3 7 3 3]\\n [3 3 3 3 3 3 3 3 3 3]]\\nOutput:\\n[[3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 7 3 7 3 7 3 3]\\n [3 3 3 3 8 3 8 3 3 3]\\n [3 3 3 7 3 0 3 7 3 3]\\n [3 3 3 3 8 3 8 3 3 3]\\n [3 3 3 7 3 7 3 7 3 3]\\n [3 3 3 3 3 3 3 3 3 3]]\\n\\nTest Input:\\n[[3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 6 3 3 3 3 3]\\n [3 3 3 8 3 8 3 3 3 3]\\n [3 3 6 3 8 3 6 3 3 3]\\n [3 3 3 8 3 8 3 3 3 3]\\n [3 3 9 3 6 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 9 3 6 3 9 3 3 3]\\n [3 3 3 8 3 8 3 3 3 3]\\n [3 3 6 3 8 3 6 3 3 3]\\n [3 3 3 8 3 8 3 3 3 3]\\n [3 3 9 3 6 3 9 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]\\n [3 3 3 3 3 3 3 3 3 3]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuning_dataset[479])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1000 examples\n",
      "{'file_name': 'a5f85a15_swap1_shuffle1_rot180', 'messages': [{'content': 'Input:\\n[[4 4 4 4 9 4 4 4]\\n [4 4 4 4 4 9 4 4]\\n [9 4 4 4 4 4 9 4]\\n [4 9 4 4 4 4 4 9]\\n [4 4 9 4 4 4 4 4]\\n [4 4 4 9 4 4 4 4]\\n [4 4 4 4 9 4 4 4]\\n [4 4 4 4 4 9 4 4]]\\nOutput:\\n[[4 4 4 4 8 4 4 4]\\n [4 4 4 4 4 9 4 4]\\n [8 4 4 4 4 4 8 4]\\n [4 9 4 4 4 4 4 9]\\n [4 4 8 4 4 4 4 4]\\n [4 4 4 9 4 4 4 4]\\n [4 4 4 4 8 4 4 4]\\n [4 4 4 4 4 9 4 4]]\\n\\nInput:\\n[[5 4 4]\\n [4 5 4]\\n [4 4 5]]\\nOutput:\\n[[5 4 4]\\n [4 8 4]\\n [4 4 5]]\\n\\nTest Input:\\n[[4 4 4 0 4 4]\\n [4 4 4 4 0 4]\\n [0 4 4 4 4 0]\\n [4 0 4 4 4 4]\\n [4 4 0 4 4 4]\\n [4 4 4 0 4 4]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[4 4 4 0 4 4]\\n [4 4 4 4 8 4]\\n [8 4 4 4 4 0]\\n [4 0 4 4 4 4]\\n [4 4 8 4 4 4]\\n [4 4 4 0 4 4]]\\n\\n', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Downsampling\n",
    "import random\n",
    "\n",
    "N = 1_000                      # how many examples you want\n",
    "seed = 42                      # for reproducibility\n",
    "\n",
    "# If you’re using a Hugging Face `datasets.Dataset`\n",
    "if hasattr(fine_tuning_dataset, \"shuffle\"):\n",
    "    fine_tuning_dataset = (\n",
    "        fine_tuning_dataset\n",
    "        .shuffle(seed=seed)            # randomise order once\n",
    "        .select(range(min(N, len(fine_tuning_dataset))))  # take the first N rows\n",
    "    )\n",
    "\n",
    "# Fallback: if it’s a plain Python list / tuple\n",
    "else:\n",
    "    random.seed(seed)\n",
    "    fine_tuning_dataset = random.sample(\n",
    "        fine_tuning_dataset, \n",
    "        k=min(N, len(fine_tuning_dataset))\n",
    "    )\n",
    "\n",
    "print(f\"Sampled {len(fine_tuning_dataset)} examples\")\n",
    "print(fine_tuning_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune with Unsloth\n",
    "- Load unsloth\n",
    "- apply lora\n",
    "- train\n",
    "- Train on outputs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.12 environment at: /usr\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mcut-cross-entropy\u001b[0m\u001b[2m==25.1.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2025.4.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2025.4.1\u001b[0m\n",
      "\u001b[2mUsing Python 3.10.12 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m78 packages\u001b[0m \u001b[2min 10.78s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K  \u001b[31m×\u001b[0m Failed to download `torch==2.7.0`8)                                                 \n",
      "\u001b[31m  ├─▶ \u001b[0mFailed to extract archive:\n",
      "\u001b[31m  │   \u001b[0mtorch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl\n",
      "\u001b[31m  ╰─▶ \u001b[0mfailed to write to file\n",
      "\u001b[31m      \u001b[0m`/tmp/.tmpVSNP6S/.tmpsUr82U/torch/lib/libtorch_cpu.so`: No space left on\n",
      "\u001b[31m      \u001b[0mdevice (os error 28)\n",
      "\u001b[36m  help: \u001b[0m`\u001b[36mtorch\u001b[39m` (\u001b[36mv2.7.0\u001b[39m) was included because `\u001b[36munsloth-zoo\u001b[39m` (\u001b[36mv2025.4.1\u001b[39m)\n",
      "        depends on `\u001b[36mtorch\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "# Unsloth can hit errors with cut cross entropy (alternative is`import os; os.environ[\"UNSLOTH_CCE_DISABLE\"] = \"1\"  # fall back to vanilla CrossEntropy`)\n",
    "# !uv pip uninstall unsloth unsloth_zoo cut-cross-entropy --system\n",
    "!uv pip install --no-cache-dir --force-reinstall --system \\\n",
    "      \"unsloth @ git+https://github.com/unslothai/unsloth.git\" \\\n",
    "      \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\" \\\n",
    "      \"cut-cross-entropy @ git+https://github.com/apple/ml-cross-entropy.git\"\n",
    "\n",
    "# !uv pip install flash_attn --no-build-isolation -qU --system # can give issues\n",
    "\n",
    "## RESTART THE KERNEL if there are issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted local  model\n",
      "deleted local  tokenizer\n",
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 20000 # 8k needed for MIT easy eval. 20k needed for full MIT training split.\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "clear_old_model_refs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model_slug = 'unsloth/Llama-3.2-1B' # can just use base model since we are simplifying the chat template anyways\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_slug,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Define the Jinja template\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "{{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "{% endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n', 'role': 'user'}, {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "[[8 6]\n",
      " [6 4]]\n",
      "Output:\n",
      "[[8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]\n",
      " [6 8 6 8 6 8]\n",
      " [4 6 4 6 4 6]\n",
      " [8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]]\n",
      "\n",
      "Input:\n",
      "[[7 9]\n",
      " [4 3]]\n",
      "Output:\n",
      "[[7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]\n",
      " [9 7 9 7 9 7]\n",
      " [3 4 3 4 3 4]\n",
      " [7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]]\n",
      "\n",
      "Test Input:\n",
      "[[3 2]\n",
      " [7 8]]\n",
      "Test Output:\n",
      "[[3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]\n",
      " [2 3 2 3 2 3]\n",
      " [8 7 8 7 8 7]\n",
      " [3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]]\n",
      "\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(eval_dataset[0]['messages'],tokenize=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_row_in_tokens(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the token length of the concatenated 'messages' content for each row\n",
    "    in the dataset and returns the length of the longest row.\n",
    "\n",
    "    Args:\n",
    "        dataset: A dataset with a 'messages' column containing lists of message dicts.\n",
    "        tokenizer: A tokenizer instance to tokenize the messages.\n",
    "\n",
    "    Returns:\n",
    "        int: The length in tokens of the longest row.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "\n",
    "    for row in dataset:\n",
    "        # Concatenate all message contents\n",
    "        concatenated_content = \" \".join(msg[\"content\"] for msg in row[\"messages\"])\n",
    "\n",
    "        # Tokenize the concatenated content and calculate its length\n",
    "        tokenized_length = len(tokenizer(concatenated_content)[\"input_ids\"])\n",
    "\n",
    "        # Track the maximum length\n",
    "        max_length = max(max_length, tokenized_length)\n",
    "\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18323"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_row_in_tokens(fine_tuning_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7423"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_row_in_tokens(eval_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # target_modules = [\"all-linear\"],\n",
    "    modules_to_save = [\"lm_head\",\"embed_tokens\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 9984\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_name', 'messages'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    \"\"\"\n",
    "    Turn every `messages` entry in a batch into a single, template-formatted string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        When `datasets.Dataset.map(..., batched=True)` is used, `examples[\"messages\"]`\n",
    "        is a list where each element is the message-history for one conversation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list whose length matches the input batch size.  Each item is the fully\n",
    "        formatted conversation produced by `tokenizer.apply_chat_template`.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tokenizer.apply_chat_template(\n",
    "            chat,                       # one conversation (list[dict])\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for chat in examples[\"messages\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Input:\\n[[8 6]\\n [6 4]]\\nOutput:\\n[[8 6 8 6 8 6]\\n [6 4 6 4 6 4]\\n [6 8 6 8 6 8]\\n [4 6 4 6 4 6]\\n [8 6 8 6 8 6]\\n [6 4 6 4 6 4]]\\n\\nInput:\\n[[7 9]\\n [4 3]]\\nOutput:\\n[[7 9 7 9 7 9]\\n [4 3 4 3 4 3]\\n [9 7 9 7 9 7]\\n [3 4 3 4 3 4]\\n [7 9 7 9 7 9]\\n [4 3 4 3 4 3]]\\n\\nTest Input:\\n[[3 2]\\n [7 8]]\\nTest Output:\\n',\n",
       "  'role': 'user'},\n",
       " {'content': '[[3 2 3 2 3 2]\\n [7 8 7 8 7 8]\\n [2 3 2 3 2 3]\\n [8 7 8 7 8 7]\\n [3 2 3 2 3 2]\\n [7 8 7 8 7 8]]\\n\\n',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "[[8 6]\n",
      " [6 4]]\n",
      "Output:\n",
      "[[8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]\n",
      " [6 8 6 8 6 8]\n",
      " [4 6 4 6 4 6]\n",
      " [8 6 8 6 8 6]\n",
      " [6 4 6 4 6 4]]\n",
      "\n",
      "Input:\n",
      "[[7 9]\n",
      " [4 3]]\n",
      "Output:\n",
      "[[7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]\n",
      " [9 7 9 7 9 7]\n",
      " [3 4 3 4 3 4]\n",
      " [7 9 7 9 7 9]\n",
      " [4 3 4 3 4 3]]\n",
      "\n",
      "Test Input:\n",
      "[[3 2]\n",
      " [7 8]]\n",
      "Test Output:\n",
      "[[3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]\n",
      " [2 3 2 3 2 3]\n",
      " [8 7 8 7 8 7]\n",
      " [3 2 3 2 3 2]\n",
      " [7 8 7 8 7 8]]\n",
      "\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(formatting_func(eval_dataset)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb676c21c3b402d805ac96fe6a5414f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/9984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7ac8a2bde048c09094d2bbe014e357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "gradient_accumulation_steps=1 # avoids small errors with ga\n",
    "batch_size=16\n",
    "epochs=2\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Calculate the total training steps\n",
    "num_training_steps = len(fine_tuning_dataset) * epochs // (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "def lr_lambda_specific(step: int):          # ← only one positional arg\n",
    "    if num_training_steps < 2:\n",
    "        return 1.0\n",
    "    half = num_training_steps // 2\n",
    "    if step < half:\n",
    "        return 1.0\n",
    "    progress = (step - half) / max(1, half)\n",
    "    return 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)).item())\n",
    "\n",
    "# Create the scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda_specific)\n",
    "\n",
    "# ----  SFTConfig replaces TrainingArguments  ----\n",
    "sft_cfg = SFTConfig(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_dir=f\"logs/{model_slug.split('/')[-1]}\"\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_steps = min(max(int(0.05 * num_training_steps), 1),10),        # use 1 not 1.0\n",
    "    eval_steps = max(int(0.1 * num_training_steps), 1),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    optim=\"adamw_torch\",               # still needed so TRL knows which states to save\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # *everything that used to live at the top level*:\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    run_name=f\"{model_slug.split('/')[-1]}-arc\",\n",
    ")\n",
    "\n",
    "# Pass optimizer and scheduler explicitly to the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=fine_tuning_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_cfg,\n",
    "    formatting_func  = formatting_func,\n",
    "    # optimizers=(optimizer, scheduler),\n",
    "    # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    ")\n",
    "\n",
    "# must be done outside because of unsloth\n",
    "trainer.optimizer     = optimizer\n",
    "trainer.lr_scheduler  = scheduler\n",
    "\n",
    "# disable the internal builders so they don't overwrite again\n",
    "trainer.create_optimizer = lambda *a, **k: trainer.optimizer\n",
    "trainer.create_scheduler = lambda *a, **k: trainer.lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f1dfee54cf4eca835f6942c23f143d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=192):   0%|          | 0/9984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 20. Reducing num_proc to 20 for dataset of size 20.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bf8c63dd274e71bed4d197e5f16008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only # or run the code above if not using unsloth\n",
    "\n",
    "# masks everything between the instruction_part and response_part\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|begin_of_text|>\",\n",
    "    response_part = \"Test Output:\\n\",\n",
    "    # force_match=False # comment out to set true for a cleaner masking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>\\nInput:\\n[[0 7 7]\\n [7 7 7]\\n [0 7 7]]\\nOutput:\\n[[0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [7 7 7 7 7 7 7 7 7]\\n [0 7 7 0 7 7 0 7 7]\\n [0 0 0 0 7 7 0 7 7]\\n [0 0 0 7 7 7 7 7 7]\\n [0 0 0 0 7 7 0 7 7]]\\n\\nInput:\\n[[4 0 4]\\n [0 0 0]\\n [0 4 0]]\\nOutput:\\n[[4 0 4 0 0 0 4 0 4]\\n [0 0 0 0 0 0 0 0 0]\\n [0 4 0 0 0 0 0 4 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 4 0 4 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 4 0 0 0 0]]\\n\\nInput:\\n[[0 0 0]\\n [0 0 2]\\n [2 0 2]]\\nOutput:\\n[[0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 2]\\n [0 0 0 0 0 0 2 0 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 2 0 0 0 0 0 2]\\n [2 0 2 0 0 0 2 0 2]]\\n\\nInput:\\n[[6 6 0]\\n [6 0 0]\\n [0 6 6]]\\nOutput:\\n[[6 6 0 6 6 0 0 0 0]\\n [6 0 0 6 0 0 0 0 0]\\n [0 6 6 0 6 6 0 0 0]\\n [6 6 0 0 0 0 0 0 0]\\n [6 0 0 0 0 0 0 0 0]\\n [0 6 6 0 0 0 0 0 0]\\n [0 0 0 6 6 0 6 6 0]\\n [0 0 0 6 0 0 6 0 0]\\n [0 0 0 0 6 6 0 6 6]]\\n\\nInput:\\n[[2 2 2]\\n [0 0 0]\\n [0 2 2]]\\nOutput:\\n[[2 2 2 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 2 2 0 2 2 0 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 2 2 2 2 2 2]\\n [0 0 0 0 0 0 0 0 0]\\n [0 0 0 0 2 2 0 2 2]]\\n\\nTest Input:\\n[[7 0 7]\\n [7 0 7]\\n [7 7 0]]\\nTest Output:\\n[[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n<|end_of_text|>'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [[7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 0 0 0 7 0 7]\\n [7 0 7 0 0 0 7 0 7]\\n [7 7 0 0 0 0 7 7 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 0 7 7 0 7 0 0 0]\\n [7 7 0 7 7 0 0 0 0]]\\n\\n<|end_of_text|>'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA H100 80GB HBM3. Max memory = 79.097 GB.\n",
      "34.037 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Check memory\n",
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, compute_cap\n",
      "NVIDIA H100 80GB HBM3, 9.0\n",
      "torch 2.6.0+cu124\n",
      "triton 3.2.0\n",
      "cut-cross-entropy 25.1.1\n",
      "unsloth 2025.4.1\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=name,compute_cap --format=csv\n",
    "\n",
    "import torch, triton, cut_cross_entropy, unsloth\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"triton\", triton.__version__)\n",
    "print(\"cut-cross-entropy\", cut_cross_entropy.__version__)\n",
    "print(\"unsloth\", unsloth.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,984 | Num Epochs = 2 | Total steps = 1,248\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 547,880,960/2,046,363,648 (26.77% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='1248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 537/1248 22:04 < 29:19, 0.40 it/s, Epoch 0.86/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.131603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.124428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let SFTTrainer handle the training and log the learning rate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m/workspace/unsloth_compiled_cache/UnslothSFTTrainer.py:748\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 748\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py:1029\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:1200\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1199\u001b[0m ):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/llama.py:1096\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bsz\u001b[38;5;241m*\u001b[39mq_len \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m: RETURN_LOGITS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RETURN_LOGITS \u001b[38;5;129;01mand\u001b[39;00m HAS_CUT_CROSS_ENTROPY \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     n_items \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_items\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1097\u001b[0m     loss \u001b[38;5;241m=\u001b[39m fused_linear_cross_entropy(\n\u001b[1;32m   1098\u001b[0m         hidden_states      \u001b[38;5;241m=\u001b[39m hidden_states,\n\u001b[1;32m   1099\u001b[0m         lm_weight          \u001b[38;5;241m=\u001b[39m lm_head,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1102\u001b[0m         logit_softcapping  \u001b[38;5;241m=\u001b[39m logit_softcapping,\n\u001b[1;32m   1103\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Let SFTTrainer handle the training and log the learning rate\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final memory results\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lora\n",
    "lora_model_name = f\"{model_slug.split('/')[-1]}-lora-model\"\n",
    "\n",
    "print(tokenizer.chat_template)\n",
    "\n",
    "model.save_pretrained(lora_model_name) # Local saving\n",
    "\n",
    "tokenizer.chat_template = None # this is a hack as Unsloth will throw an error when reloading the given tokenizer. So we need to manually apply the tokenizer after model loading\n",
    "\n",
    "tokenizer.save_pretrained(lora_model_name) # Local saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = \"Trelis\"\n",
    "print(f\"Pushing as {run_name} and {org}/{lora_model_name}\")\n",
    "\n",
    "# push adapters to hub\n",
    "model.push_to_hub(f\"{org}/{lora_model_name}\")\n",
    "tokenizer.push_to_hub(f\"{org}/{lora_model_name}\")\n",
    "\n",
    "# # merge and push to hub    \n",
    "# model.push_to_hub_merged(f\"{org}/{lora_model_name}\", tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Jinja template\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "{{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "{% endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate using the base or fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Set the model ---\n",
    "# # del model, tokenizer\n",
    "\n",
    "# # model_slug = 'unsloth/Llama-3.2-1B' # using a base or instruct model won't work as there are no instructions on answer format provided in the prompt (to keep things succinct).\n",
    "# # model_slug = 'Trelis/Llama-3.2-1B-lora-model' # for the fine-tuned model\n",
    "# model_slug = 'Llama-3.2-1B-lora-model'\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# import torch\n",
    "# max_seq_length = 8000 # 8k needed for easy. 20k needed for full MIT split\n",
    "# dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_slug,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "# )\n",
    "\n",
    "# # Define the Jinja template\n",
    "# tokenizer.chat_template = \"\"\"\n",
    "# {% for message in messages %}\n",
    "# {{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "# {% endfor %}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def verify_vs_ground_truth(generated_output_text, correct_output, visualize=False):\n",
    "    \"\"\"Compares generated and correct outputs, handling diverse formats.\"\"\"\n",
    "    try:\n",
    "        # Preprocess generated output and correct output\n",
    "        def preprocess_output(output_text):\n",
    "            # Replace spaces between numbers with commas\n",
    "            cleaned_text = re.sub(r\"(?<=\\d)\\s+(?=\\d)\", \",\", output_text.strip())\n",
    "            # Replace newlines and spaces between rows with commas\n",
    "            cleaned_text = re.sub(r\"\\]\\s*\\[\", \"],[\", cleaned_text)\n",
    "            return cleaned_text\n",
    "\n",
    "        # Preprocess both generated and correct outputs\n",
    "        cleaned_generated_output = preprocess_output(generated_output_text)\n",
    "        cleaned_correct_output = preprocess_output(correct_output)\n",
    "\n",
    "        # Parse the cleaned outputs into NumPy arrays\n",
    "        processed_output = np.array(eval(cleaned_generated_output))\n",
    "        correct_output_array = np.array(eval(cleaned_correct_output))\n",
    "\n",
    "        if visualize:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            plot_grid(processed_output,  'Generated Output',  ax=axes[0])\n",
    "            plot_grid(correct_output_array, 'Correct Output', ax=axes[1])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Validate shapes and broadcast if necessary\n",
    "        if processed_output.shape != correct_output_array.shape:\n",
    "            try:\n",
    "                processed_output = np.broadcast_to(processed_output, correct_output_array.shape)\n",
    "            except ValueError:\n",
    "                print(f\"Shape mismatch: Generated {processed_output.shape} vs Expected {correct_output_array.shape}\")\n",
    "                return False, 0.0, [[0]]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        total_pixels = correct_output_array.size\n",
    "        correct_pixels = np.sum(processed_output == correct_output_array)\n",
    "        accuracy = (correct_pixels / total_pixels) * 100\n",
    "        is_correct = correct_pixels == total_pixels\n",
    "\n",
    "        return is_correct, accuracy, processed_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing: {e}, Generated Output: {generated_output_text}\")\n",
    "        return False, 0.0, [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def solve_challenge_unsloth(messages, model, tokenizer, visualize=False):\n",
    "    \"\"\"Solves a single ARC challenge using unsloth for inference.\"\"\"\n",
    "\n",
    "    # print(messages)\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # prompt = tokenizer.apply_chat_template(\n",
    "    #     messages[:-1],\n",
    "    #     tokenize=False,\n",
    "    #     add_generation_prompt=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "\n",
    "    # print(prompt)\n",
    "    \n",
    "    try:\n",
    "        # Enable faster inference\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        \n",
    "        outputs = model.generate(input_ids = inputs,\n",
    "                                 max_new_tokens = 1500, # 1,500 for MIT easy.\n",
    "                                 use_cache = True,\n",
    "                                 temperature = 0.01)\n",
    "\n",
    "        generated_output = outputs[:, inputs.shape[1]:]\n",
    "\n",
    "        # Decode the output\n",
    "        generated_output_text = tokenizer.batch_decode(generated_output, skip_special_tokens=True)[0]\n",
    "\n",
    "        # print(f\"Generated output text:\\n{generated_output_text}\")\n",
    "\n",
    "        # print(f\"Ground Truth:\\n{messages[-1]['content']}\")\n",
    "        \n",
    "        # Verify the generated output\n",
    "        is_correct, accuracy, processed_output = verify_vs_ground_truth(\n",
    "            generated_output_text, messages[-1]['content'], visualize=visualize\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'file_name': challenge['file_name'],\n",
    "            'generated_output': generated_output_text,\n",
    "            'is_correct': is_correct,\n",
    "            'accuracy': accuracy,\n",
    "            'processed_output': processed_output,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing challenge {challenge['file_name']}: {e}\")\n",
    "        return {\n",
    "            'file_name': challenge['file_name'],\n",
    "            'generated_output': \"Error\",\n",
    "            'is_correct': False,\n",
    "            'accuracy': 0.0,\n",
    "            'processed_output': [[0]],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 20\n",
    "\n",
    "dataset_to_run = eval_dataset.select(\n",
    "    # range(num_tests-3,num_tests)\n",
    "    range(num_tests)\n",
    ")\n",
    "\n",
    "print(dataset_to_run)\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    result = solve_challenge_unsloth(challenge['messages'], model, tokenizer, visualize=True)\n",
    "    results.append(result)\n",
    "\n",
    "    # Update the correct count if the result is correct\n",
    "    if result.get('is_correct', False):\n",
    "        correct_count += 1\n",
    "\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} complete. Correct so far: {correct_count}/{i+1}.\\n---\\n\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Depth-first Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_old_model_refs()\n",
    "\n",
    "# model_slug = 'unsloth/Llama-3.2-1B' # using a base or instruct model won't work as there are no instructions on answer format provided in the prompt (to keep things succinct).\n",
    "# model_slug = 'Trelis/Llama-3.2-1B-lora-model' # for the fine-tuned model\n",
    "model_slug = 'Llama-3.2-1B-lora-model'\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" #for fast weight downloads\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 8000 # 8k needed for easy. 20k needed for full MIT split\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_slug,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Define the Jinja template\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "{{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "{% endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------ #\n",
    "# batched DFS helper (identical to previous post)                          #\n",
    "# ------------------------------------------------------------------------ #\n",
    "import math, heapq, torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "import math, heapq, torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def dfs_candidates(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_ids: torch.LongTensor,\n",
    "    max_tokens: int,\n",
    "    prob_thresh: float = 0.05,\n",
    "    top_k: int = 5,\n",
    "    batch_size: int = 64,\n",
    ") -> List[Tuple[torch.Tensor, float]]:\n",
    "    \"\"\"\n",
    "    Return list of (sequence_tensor, cumulative_logP) sorted best-first.\n",
    "    \"\"\"\n",
    "    device  = prompt_ids.device\n",
    "    eos_id  = tokenizer.eos_token_id\n",
    "    log_cut = math.log(prob_thresh)\n",
    "\n",
    "    # (priority, counter, tensor) – counter breaks ties\n",
    "    counter   = 0\n",
    "    frontier  = [(-0.0, counter, prompt_ids)]\n",
    "    solutions = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while frontier:\n",
    "            batch = [heapq.heappop(frontier)\n",
    "                     for _ in range(min(batch_size, len(frontier)))]\n",
    "            _, _, seqs = zip(*batch)\n",
    "            lens    = torch.tensor([s.shape[1] for s in seqs], device=device)\n",
    "\n",
    "            Lmax = int(lens.max())\n",
    "            padded = torch.full((len(seqs), Lmax),\n",
    "                                tokenizer.pad_token_id,\n",
    "                                device=device)\n",
    "            for i, s in enumerate(seqs):\n",
    "                padded[i, : s.shape[1]] = s\n",
    "\n",
    "            next_lp = torch.log_softmax(\n",
    "                model(padded).logits[torch.arange(len(seqs)), lens - 1],\n",
    "                dim=-1)\n",
    "\n",
    "            for b, prefix in enumerate(seqs):\n",
    "                base_lp = -batch[b][0]                    # recover +logP\n",
    "                vals, idxs = next_lp[b].topk(top_k)\n",
    "\n",
    "                for tok_lp, tok_id in zip(vals.tolist(), idxs.tolist()):\n",
    "                    new_lp = base_lp + tok_lp\n",
    "                    if new_lp < log_cut:\n",
    "                        break\n",
    "\n",
    "                    child = torch.cat(\n",
    "                        [prefix,\n",
    "                         torch.tensor([[tok_id]], device=device)], dim=1)\n",
    "\n",
    "                    done = (tok_id == eos_id or\n",
    "                            child.shape[1] - prompt_ids.shape[1] >= max_tokens)\n",
    "                    if done:\n",
    "                        solutions.append((child.clone(), new_lp))\n",
    "                    else:\n",
    "                        counter += 1\n",
    "                        heapq.heappush(frontier, (-new_lp, counter, child))\n",
    "\n",
    "    solutions.sort(key=lambda x: -x[1])\n",
    "    return solutions\n",
    "\n",
    "# ------------------------------------------------------------------------ #\n",
    "\n",
    "# ------------------------------------------------------------------------ #\n",
    "# tiny helper – always return a plain nested list                          #\n",
    "# ------------------------------------------------------------------------ #\n",
    "def grid_as_list(arr):\n",
    "    if arr is None:\n",
    "        return [[0]]\n",
    "    return arr.tolist() if hasattr(arr, \"tolist\") else arr\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------ #\n",
    "# replacement for solve_challenge_unsloth                                  #\n",
    "# ------------------------------------------------------------------------ #\n",
    "def solve_challenge_unsloth_dfs_two_attempts(\n",
    "        messages,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        visualize=False,\n",
    "        *,\n",
    "        prob_thresh: float = 0.05,\n",
    "        top_k: int = 5,\n",
    "        batch_size: int = 64,\n",
    "        max_tokens: int = 1500,\n",
    "        verbose: bool = True,\n",
    "):\n",
    "    # ── build prompt ────────────────────────────────────────────────────\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # ── 1. enumerate candidates with DFS ────────────────────────────────\n",
    "    cands = dfs_candidates(\n",
    "        model, tokenizer, prompt_ids,\n",
    "        max_tokens=max_tokens,\n",
    "        prob_thresh=prob_thresh,\n",
    "        top_k=top_k,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[DFS] {len(cands)} candidates ≥ {prob_thresh:.2%}\")\n",
    "\n",
    "    # ── 2. if nothing survived, return two blank grids ───────────────────\n",
    "    if not cands:\n",
    "        blank = [[0]]\n",
    "        return {\n",
    "            \"generated_output\": \"[]\",\n",
    "            \"is_correct\": False,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"attempt_1\": blank,\n",
    "            \"attempt_2\": blank,\n",
    "        }\n",
    "\n",
    "    # ── 3. sort purely by log-prob (highest first) ───────────────────────\n",
    "    cands.sort(key=lambda x: -x[1])                 # x[1] = cumulative logP\n",
    "\n",
    "    # first distinct grid = attempt-1\n",
    "    attempt_1_ids, _ = cands[0]\n",
    "    attempt_1_text = tokenizer.decode(\n",
    "        attempt_1_ids[:, prompt_ids.shape[1]:][0],\n",
    "        skip_special_tokens=True)\n",
    "    attempt_1_arr = None      # we’ll parse after selection\n",
    "\n",
    "    # second distinct grid (if any) = attempt-2\n",
    "    attempt_2_text, attempt_2_arr = attempt_1_text, None\n",
    "    for ids, _ in cands[1:]:\n",
    "        txt = tokenizer.decode(\n",
    "            ids[:, prompt_ids.shape[1]:][0],\n",
    "            skip_special_tokens=True)\n",
    "        if txt != attempt_1_text:\n",
    "            attempt_2_text = txt\n",
    "            break                                   # found distinct\n",
    "    # parse both into arrays once (for optional visualisation / saving)\n",
    "    ok1, acc1, arr1 = verify_vs_ground_truth(\n",
    "        attempt_1_text, messages[-1]['content'], visualize=visualize)\n",
    "    ok2, acc2, arr2 = verify_vs_ground_truth(\n",
    "        attempt_2_text, messages[-1]['content'], visualize=visualize)\n",
    "\n",
    "    is_correct = ok1 or ok2\n",
    "\n",
    "    return {\n",
    "        \"generated_output\": attempt_1_text,         # text of attempt-1\n",
    "        \"is_correct\":       is_correct,             # perfect via either attempt\n",
    "        \"accuracy\":         acc1,                   # accuracy of attempt-1 only\n",
    "        \"attempt_1\":        grid_as_list(arr1),\n",
    "        \"attempt_2\":        grid_as_list(arr2),\n",
    "    }\n",
    "# ------------------------------------------------------------------------ #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 20\n",
    "\n",
    "dataset_to_run = eval_dataset.select(\n",
    "    # range(num_tests-3,num_tests)\n",
    "    range(num_tests)\n",
    ")\n",
    "\n",
    "print(dataset_to_run)\n",
    "\n",
    "results = []\n",
    "correct_count = 0\n",
    "i=0\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):\n",
    "    res = solve_challenge_unsloth_dfs_two_attempts(\n",
    "            challenge['messages'],\n",
    "            model,\n",
    "            tokenizer,\n",
    "            visualize=True)\n",
    "    results.append(res)\n",
    "\n",
    "    if res[\"is_correct\"]:\n",
    "        correct_count += 1\n",
    "    print(f\"Challenge {i+1}/{len(dataset_to_run)} \"\n",
    "          f\"correct so far: {correct_count}/{i+1}\\n\\n\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Challenge-specific fine-tuning\n",
    "To run this, your fine-tuning examples MUST be from the evaluation dataset (by definition, this is what ttt is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_to_ttft ='Llama-3.2-1B-lora-model'\n",
    "model_to_ttft = 'unsloth/Llama-3.2-1B' # using a base or instruct model won't work as there are no instructions on answer format provided in the prompt (to keep things succinct).\n",
    "\n",
    "# model_slug = 'Trelis/Llama-3.2-1B-lora-model' # for the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training set (lots of augmentation, no test examples)\n",
    "fine_tuning_dataset = prepare_fine_tuning_dataset(\n",
    "    # arc_train, # or set to arc_eval with omit_test=True\n",
    "    arc_eval,\n",
    "    omit_test=True,\n",
    "    add_shuffled=1, #set to True for all permutations, or set to an integer for a max number of shuffles\n",
    "    add_rotations=True, # applies to original rotation only\n",
    "    add_mirrors=True, # applies to original examples only\n",
    "    apply_color_swaps=True,\n",
    "    num_color_swaps=1,\n",
    ")\n",
    "\n",
    "print(fine_tuning_dataset)\n",
    "print(fine_tuning_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fine_tuning_dataset(fine_tuning_dataset, target_file_name):\n",
    "    \"\"\"\n",
    "    Filters the fine-tuning dataset to include only rows corresponding to the specified file name.\n",
    "\n",
    "    Args:\n",
    "        fine_tuning_dataset: A HuggingFace Dataset containing expanded fine-tuning data.\n",
    "        target_file_name: A string specifying the file name to filter by.\n",
    "\n",
    "    Returns:\n",
    "        A HuggingFace Dataset containing rows only for the specified file name.\n",
    "    \"\"\"\n",
    "    # Keep any example whose file name *contains* the target string\n",
    "    filtered_data = [\n",
    "        example for example in fine_tuning_dataset\n",
    "        if target_file_name in example[\"file_name\"]      # substring match\n",
    "    ]\n",
    "    \n",
    "    # Create a HuggingFace Dataset from the filtered data\n",
    "    filtered_dataset = Dataset.from_list(filtered_data)\n",
    "\n",
    "    print(f\"Filtering to only include {target_file_name}\")\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    \"\"\"\n",
    "    Turn every `messages` entry in a batch into a single, template-formatted string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    examples : dict\n",
    "        When `datasets.Dataset.map(..., batched=True)` is used, `examples[\"messages\"]`\n",
    "        is a list where each element is the message-history for one conversation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list whose length matches the input batch size.  Each item is the fully\n",
    "        formatted conversation produced by `tokenizer.apply_chat_template`.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        tokenizer.apply_chat_template(\n",
    "            chat,                       # one conversation (list[dict])\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for chat in examples[\"messages\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "batch_size=1\n",
    "epochs=2\n",
    "\n",
    "completions_only=True\n",
    "\n",
    "results       = []          # reset every run\n",
    "correct_count = 0           # reset every run\n",
    "\n",
    "# Select the range of challenges to run\n",
    "dataset_to_run = eval_dataset\n",
    "\n",
    "for i, challenge in enumerate(dataset_to_run):  # Iterate directly through the dataset\n",
    "    # Step 1: Filter the fine-tuning dataset for the current challenge\n",
    "    challenge_file_name = challenge['file_name']\n",
    "\n",
    "    challenge_dataset = filter_fine_tuning_dataset(fine_tuning_dataset, challenge_file_name)\n",
    "\n",
    "    print(challenge_dataset)\n",
    "    \n",
    "    # print(challenge_file_name)\n",
    "\n",
    "    clear_old_model_refs()\n",
    "    \n",
    "    # Step 2: Reload the model with the base LoRA\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_to_ttft,  # Replace with your model's name\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable faster inference\n",
    "\n",
    "    # Define the Jinja template - Necessary as Unsloth won't accept our tokenizer\n",
    "    tokenizer.chat_template = \"\"\"\n",
    "    {% for message in messages %}\n",
    "    {{ message.content }}{% if message.role == 'assistant' %}{{ eos_token }}{% endif %}{%- if add_generation_prompt %}{%- endif %}\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "\n",
    "    # print(f\"Dataset size: {len(challenge_dataset)}\")\n",
    "    # print(f\"Epochs: {epochs}\")\n",
    "    # print(f\"Batch size: {batch_size}\")\n",
    "    # print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "    # print(f\"Computed num_training_steps: {num_training_steps}\")\n",
    "\n",
    "    # Step 3: Fine-tune the model on the challenge-specific dataset\n",
    "    # Define the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    # Calculate the total training steps\n",
    "    num_training_steps = len(fine_tuning_dataset) * epochs // (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "    def lr_lambda_specific(step: int):          # ← only one positional arg\n",
    "        if num_training_steps < 2:\n",
    "            return 1.0\n",
    "        half = num_training_steps // 2\n",
    "        if step < half:\n",
    "            return 1.0\n",
    "        progress = (step - half) / max(1, half)\n",
    "        return 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)).item())\n",
    "    \n",
    "    # Create the scheduler\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda_specific)\n",
    "    \n",
    "    # ----  SFTConfig replaces TrainingArguments  ----\n",
    "    sft_cfg = SFTConfig(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_strategy=\"steps\",\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_steps = 1,\n",
    "        eval_steps = 0.2,\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        optim=\"adamw_torch\",               # still needed so TRL knows which states to save\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"tensorboard\",\n",
    "        # *everything that used to live at the top level*:\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=2,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    # Pass optimizer and scheduler explicitly to the trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=challenge_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=sft_cfg,\n",
    "        formatting_func  = formatting_func,\n",
    "        # data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    )\n",
    "    \n",
    "    trainer.optimizer     = optimizer\n",
    "    trainer.lr_scheduler  = scheduler\n",
    "\n",
    "    # disable the internal builders so they don't overwrite again\n",
    "    trainer.create_optimizer = lambda *a, **k: trainer.optimizer\n",
    "    trainer.create_scheduler = lambda *a, **k: trainer.lr_scheduler\n",
    "\n",
    "    if completions_only:\n",
    "        print(f\"TRAINING ON COMPLETIONS ONLY!\")\n",
    "        # Requires commenting in the datacollator above in the trainer.\n",
    "        from unsloth.chat_templates import train_on_responses_only # or run the code above if not using unsloth\n",
    "        \n",
    "        # masks everything between the instruction_part and response_part\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = \"<|begin_of_text|>\",\n",
    "            response_part = \"Test Output:\\n\",\n",
    "            # force_match=False # comment out to set true for a cleaner masking\n",
    "        )\n",
    "        \n",
    "        tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])\n",
    "        \n",
    "        space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "        tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])\n",
    "\n",
    "    trainer.train()  # Fine-tune the model\n",
    "\n",
    "    # Step 4: Run inference on the fine-tuned model\n",
    "    result = solve_challenge_unsloth_dfs_two_attempts(\n",
    "                challenge['messages'],\n",
    "                model,\n",
    "                tokenizer,\n",
    "                visualize=True)      # turn off when you don’t want the plots\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # bookkeeping exactly as before\n",
    "    if result[\"is_correct\"]:\n",
    "        correct_count += 1\n",
    "    print(f\"✓ task {i+1}: tally = {correct_count}/{i+1}\\n\\n\")\n",
    "\n",
    "# Final tally\n",
    "print(f\"Final Tally: {correct_count}/{len(dataset_to_run)} challenges correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5123959,
     "sourceId": 8622192,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 89194,
     "modelInstanceId": 64831,
     "sourceId": 77114,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 91102,
     "modelInstanceId": 68809,
     "sourceId": 104449,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
